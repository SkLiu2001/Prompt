近年来，以机器学习，尤其是深度学习，为
代表的人工智能技术在图像识别[1]、语音识别[2]、
机器翻译[3]和自动驾驶[4]等应用领域都取得了突
破性进展。其原因可以分为三个方面：首先，互
联网、大数据技术的发展，积累了海量的训练数
据，在这些数据中蕴含着丰富的信息；其次，机
器学习理论的发展使得机器学习算法和模型不断
完善，为从训练数据中挖掘有价值的信息创造可
能；最后，GPU 等加速器以及云计算等技术大幅
提升计算性能，大大加快了从数据中获取信息的
速度，使得机器学习技术的应用成为现实。总之，
作为第三次人工智能浪潮的“催化剂”，算力的大
幅提升直接将人工智能再次推向新的繁荣期。
高性能的机器学习算法往往具有更高的计算
需求。据 OpenAI 统计，人工智能训练所需要的算
力呈指数级增长，每 3.5 个月翻一倍①。相比之下，
近年来计算引擎的发展速度则远远落后于模型计
算需求的增长。以 Nvidia GPU 发展为例，表 1 展
示了 2012 年以来 Nvidia 的多代 GPU 在训练
ResNet 模型时的性能表现。可以看到近 8 年来，
GPU 的计算性能只提高了 16 倍左右，远低于同
期模型计算需求的增长。在“后摩尔定律”时代，
单个计算引擎的性能提升逐渐进入了瓶颈期。面
对日益复杂的计算任务，分布式机器学习被认为
是必然的发展趋势，逐渐成为业界的研究热点[5]。
 
① AI and Compute. https://openai.com/blog/ai-and-compute/ 201
8,05,16
王帅等：分布式机器学习系统网络性能优化研究进展 3
表 1 单 GPU 训练 ResNet-269 模型的性能
GPU 型号 发布时间 训练速度（图片/秒）
K520 2012 年 4
K80 2014 年 11
M60 2015 年 14
1080Ti 2017 年 44
V100 2019 年 65
在分布式机器学习训练任务的迭代计算过程
中，不同计算节点间需要频繁同步机器学习模型
参数，以使得该模型能够遍历完整的数据集，从
而保证最终得到的模型与使用单机训练的模型一
致。然而，随着计算节点数量的增多，一方面，
不同节点间进行参数同步的流量逐渐增加；另一
方面，为了避免单轮训练过多数据（即批尺寸过
大）带来的模型泛化能力下降问题[6]，每个节点
所分配的计算任务会逐渐减少。因此，对于分布
式机器学习系统，通信开销和计算开销的比值会
随着系统规模的增大而呈现幂增长趋势。这导致
通信成为限制大规模分布式机器学习系统扩展效
率的主要瓶颈，甚至出现随着节点数量增加，模
型训练速度反而下降的情况[7-11]。并且，过多的通
信时间会导致 GPU 等昂贵的计算设备大部分时
间处于等待参数同步的状态，造成计算资源的浪
费。因此，研究如何对分布式机器学习系统的网
络性能进行优化，降低通信操作对分布式机器学
习系统扩展效率的影响，从而提高机器学习模型
的训练速度，具有重要的研究意义和实用价值。
本文将首先介绍分布式机器学习系统的通信
特点，并分析网络通信成为分布式机器学习系统
扩展性瓶颈的原因，然后提出三种优化网络性能
的关键思路，并以这些思路为指导，从通信模式、
通信数据量、通信效率以及网络拓扑等方面具体
地介绍分布式机器学习系统网络性能优化研究的
最新进展，并从加速效果、优化机制、扩展性、
对模型收敛性的影响以及是否需要升级硬件设备
或更新互联方式等多个角度对这些研究工作进行
对比分析，最后讨论分布式机器学习系统中网络
性能优化研究的未来发展趋势。
2 背景介绍
2.1 机器学习概述
机器学习是一门交叉学科，涉及概率统计、
决策论、计算机科学等多个学科。概括地说，机
器学习研究的是如何使计算机模拟人类的学习行
为：从历史经验（即，数据）中总结出规律（即，
模型），并将该规律应用到新的场景中。从数据中
学习到模型的过程称作训练，在新的场景中使用
学习到的模型进行预测的过程称作推理。通常，
为了提高机器学习模型的质量，需要以海量数据
作为模型训练的输入，并且需要反复的学习。相
比之下，推理过程则只需要将新场景作为输入，
机器学习模型便会输出推理结果。由此可见，训
练过程比推理过程更加耗时。
作为机器学习模型的主要代表，人工神经网
络是一种模拟生物神经网络的人工智能架构，由
大量的人工神经元相互连接而构成。人工神经元
是一个接收输入信号并转换为输出信号的基本单
元。如图 1 所示，
xi
表示输入信号，由训练数据
或其他神经元 i 产生，
wij
表示神经元 i 和神经元
j 之间的连接权重，
bj
表示神经元偏置，
y j
表示
输出信号。图 1 对应的数学表达为
1
n
j i ij
i
y x  w bj

 
      
 
(1)
其中，
σ·
为非线性激活函数。神经网络中
最重要的是神经元之间的连接权重和结构。神经
网络的训练过程便是不断地调整权重，使其趋于
最佳的过程，因此权重也被称作参数。不同于权
重，神经网络结构一般需要人为设计，并且在训
练过程中保持不变。神经网络通常有多层神经元
组成。图 2 是一个 4 层神经网络示例，包含 1 个
输入层、2 个隐含层和 1 个输出层。在实际应用
中，神经网络的深度正在不断增加。例如，ResNet
模型[12]的深度高达 152 层。
 
 
…
…
 
 
 
 
Σ
 
 
 
w11 w1 2 w1 3 w23
输入层 隐 含层 隐 含层 输 出层
图 1 神经元模型 图 2 多层神经网络
模型在训练过程中需要使用优化器对权重进
行优化。常用的优化器主要包括随机梯度下降
（Stochastic Gradient Descent，SGD）①和标准动
 
① 现在 SGD 一般指小批量梯度下降（Mini-Batch Gradient Descent， MBGD）。
4 计 算 机 学 报
量优化（Momentum）等。下面以 SGD 优化器为
例介绍模型参数更新过程。模型训练分为很多轮，
并且每轮只以一小批样本作为输入，这样可以降
低参数更新时的方差，使得收敛更稳定。假设第
t 轮训练时模型参数为
t 
，模型对应的函数为
J ( ; ) 
t ·
，单轮训练输入的样本数量，即批尺寸
（batch size），为 n，第 i 个样本为
xi
，则 SGD 的
更新公式如下：
 
1
1
1
;
n
t t t
i
i
J x
n
   


   
(2)
其中，

表示学习率，
J (·)
表示新样本对
模型参数的影响，即梯度。随着训练的进行，参
数会朝着使模型误差（即损失）减小的方向不断
更新。
机器学习模型在每轮训练时，需要依次执行
两个计算阶段：前向计算（Forward Propagation，
FP）和反向计算（Backward Propagation，BP）。
前向计算阶段使用当前的模型参数对输入的样本
按照从输入层到输出层的顺序逐层计算，得到当
前模型对于该样本的预测，然后和预期输出作比
较，得到当前模型的损失。反向计算阶段与前向
计算阶段方向恰好相反，首先，输出层以损失值
为输入，结合前向计算阶段产生的激活值（即各
层计算的中间结果），计算出该层参数对应的梯
度，然后继续向前传播，直至所有层都计算出梯
度。最后，根据公式（2）对模型参数进行更新，
从而得到新的模型供下一轮训练使用。
2.2 分布式机器学习
随着信息技术快速发展，全球数据呈现爆发
式增长，推动人类社会迈入大数据时代。在大数
据时代，机器学习训练任务往往需要对海量的训
练数据进行大量的计算，以提高模型的准确度。
在单机上执行这样的训练任务，无论是在计算速
度还是在数据存储方面都显得十分吃力。例如，
使用单块 Nvidia Tesla V100 GPU 训练自然语言处
理模型 BERT-large 需要耗时 78 天，这显然是不可
接受的。分布式机器学习的目标则是将训练任务
分布式地部署到多个计算节点，从而提高模型训
练的速度，减少任务耗时。因此，分布式机器学
习已经成为机器学习最热门的研究领域之一。
分布式机器学习的并行方式主要包括数据并
行（Data Parallelism）和模型并行（Model Parallelism）。如图 3 所示，数据并行是指每个计算节
点上均具有同一机器学习模型的副本，但不同计
算节点分配到的训练数据是不同的，不同计算节
点间需要将各自的模型更新进行同步，以保证机
器学习模型的全局一致性。模型并行则是将机器
学习模型划分为多个子模型，并分别部署在不同
计算节点上，训练数据统一输入，前一节点完成
子模型计算后将计算结果传递给后一节点继续对
该训练样本进行处理。数据并行由于操作简单，
且不同节点的计算负载比较均衡，应用最为广泛。
目前，TensorFlow[13]、Pytorch[14]和 MXNet[15]等主
流机器学习框架均对数据并行提供了支持，并且
具有极好的易用性，但模型并行仍需要用户手动
对模型进行划分和分布式部署。
训练
数据
节点1 节点2
节点3 节点4
训练
数据
节点1 节点2
节点3
节点4
(a) 数据并行 (b) 模型并行
图 3 典型的分布式机器学习并行方式
假设某分布式机器学习系统共有 m 个计算节
点，批尺寸为 n。当采用数据并行时，每个计算
节点每轮处理的样本数量为 n/m。各计算节点基
于所分配到的训练数据对模型进行更新，然后将
不同计算节点更新后的模型进行汇总。汇总后的
模型参数如下：
1
1 1,
0
( 1) 1
0
1
1
1
1 1 ( ( ; ))
/
1
( ; )
m
t t w
w n
w
m m
t t
i
w n
i w
m n
t t
i
i
m
J x
m n m
J x
n
 
  
  

 




 


  
  

 

(3)
对比公式(2)和公式(3)可知，如果在每一轮训
练结束时，将模型参数在所有计算节点间进行同
步，则分布式训练时的模型参数变化和单机训练
完全相同，即分布式训练可以在不改变模型的收
敛性的前提下提高模型的收敛速度。需要说明的
是，在实际应用中，一般是对不同计算节点的梯
度进行汇总，然后使用汇总后的梯度来更新模型
参数，并将新的参数赋给各计算节点的模型副本。
相比于单机训练，分布式训练额外引入了节
点之间的数据通信，从而导致分布式训练的速度
无法随着计算节点数量的增加而线性提高。衡量
王帅等：分布式机器学习系统网络性能优化研究进展 5
分布式机器学习训练加速效果的指标主要包括加
速比（speedup）和扩展效率（scaling efficiency）。
加速比是指同一机器学习训练任务在单机训练和
分布式训练时所需时间的比值。加速比越大，分
布式训练的加速效果越显著，也就是说，可以更
快地完成训练任务。需要注意的是，加速比有可
能出现小于 1 的情况，此时分布式训练速度反而
不及单机训练。扩展效率是指加速比和计算节点
数量的比值。扩展效率越高，各计算节点的计算
资源利用率也就越高。图 4 展示了利用 Nvidia
Tesla V100 GPU 训练 BERT 模型时加速比和扩展
效率随 GPU 数量的变化，可以看出加速比和扩展
效率的变化趋势并不相同。一般来说，随着节点
数量的增多，扩展效率呈下降趋势，而加速比则
呈先升后降趋势。加速比在节点数量增多时反而
下降，是由于参数同步引入的通信开销抵消了新
增节点带来的性能收益。
80
85
90
95
100
0
2
4
6
8
1 4 8 扩展效率 (%)
加速比
GPU数量
加速比 扩展效率
图 4 分布式机器学习系统的扩展性能
具体来说，网络性能之所以会成为分布式机
器学习系统扩展性瓶颈的主要原因有以下三点：
1） 机器学习模型越来越复杂，模型参数量
不断增加。机器学习算法理论的快速发展催生出
各种各样的机器学习模型。例如，OpenAI 最近提
出的自然语言处理模型 GPT-3
[16]具有 1750 亿参
数，而 Krizhevsky 等 人 在 2012 年提出的
AlexNet[17]模型参数量仅为 0.45 亿。这导致在相
同节点规模下，任意两个计算节点间需要同步的
参数量随之大幅增加，加剧了分布式机器学习系
统中网络通信的压力；
2） 复杂的机器学习模型同时也意味着需要
更多的算力，导致分布式系统规模的增大。如前
所述，在机器学习模型不变的情况下，分布式系
统规模越大，每个节点所承担的计算任务越少，
计算耗时越短；与之相反，分布式系统规模越大，
每个节点需要通信的对端节点数量越多，通信耗
时越长。因此，随着分布式机器学习系统规模的
增大，通信开销在整体模型训练开销中的占比越
来越高；
3） GPU 等计算设备的性能提升速度快于网
络设备的升级。虽然计算设备的性能提升速度远
不及模型算力需求的增长，但仍比网络设备的升
级速度更快。计算性能和通信性能的差距越来越
大，即，计算资源在单位时间内处理的数据需要
更长的时间才能被网络资源处理完。这意味着分
布式机器学习系统中的网络瓶颈问题将会日益严
重。
传输协议 TCP RDMA NVLink MLT GDR
网络拓扑 Fat-Tree BCube Torus BiGraph Lotus
并行方式 数据并行 模型并行 流水并行 混合并行
一致性协议 BSP ASP SSP
同步架构 PS Ring Allreduce HD Allreduce HiPS
降
低
通
信
需
求
提
升
通
信
能
力
提
高
通
信
效
率
Gossip
通信调度 TicTac P3 ByteScheduler Geryon CEFS
网内聚合 SwitchML SHARP PBox
集合通信 NCCL bitAllReduce
MG-WFBP
模型压缩 参数量化 参数稀疏化 模型剪枝 知识蒸馏
MPI Horovod
图 5 分布式机器学习系统网络性能优化工作分类
如图 5 所示，为消除（或缓解）网络性能对
分布式机器学习系统扩展性的限制，从本质上来
说，有以下三种根本性思路：
1） 降低通信需求。通信需求，即需要通信
的数据量和通信次数，对通信耗时具有根本性的
影响。显然，数据量越大，或者通信越频繁，通
信耗时越长。因此，为了减少通信耗时，可以从
机器学习算法层面构建低网络通信需求的训练模
型，或采用知识蒸馏和模型剪枝等方式对原模型
进行修改以减小模型尺寸。这些方法会造成训练
模型的变化，超出了本文的讨论范围，故后文不
再作详细描述。参数量化以及参数稀疏化保持训
练模型不变，通过降低被传输的参数量来降低通
信需求。并行方式的优化通过权衡参数数据量和
激活值数据量的大小来切换不同的并行方式：当
参数数据量较少时，采用数据并行；反之，采用
模型并行。模型一致性协议通过控制参数同步的
频率来调节通信需求。参数同步架构对通信次数
和每次通信的数据量均会产生影响，通过选取合
适的参数同步架构可以有效降低通信需求；
2） 提升通信能力。在通信需求一定时，分
布式系统的通信能力越高，通信耗时越短。通信
能力的提升主要有两种方式。一种是利用 RDMA、
6 计 算 机 学 报
NVLink 等高性能传输协议实现高带宽、低时延的
网络传输，或利用 MLT 等新型机器学习专用传输
协议降低丢包对传输性能的影响；另一种是采用
高带宽的网络互联拓扑。例如，BCube 和 BiGraph
均采用多网卡服务器架构，不但大幅提高每个计
算节点对外通信的能力，甚至可将节点内的通信
流量导出到服务器外部，从而绕过 PCIe 瓶颈；
3） 提高通信效率。在通信需求和通信能力
均确定的情况下，还可以通过提高通信效率来加
速分布式机器学习训练。例如，在 GPU 节点间进
行集合通信时，NCCL 通信库由于针对 GPU 设备
采取了定制优化，因此具有比传统集合通信库
MPI 更高的性能。网内聚合通过逐跳汇聚参数，
增加了单位数据所蕴含的参数信息，从而提高了
通信效率。通信调度是在通信需求和通信能力固
定的情况下，高效利用网络资源的一种方式，既
包括采用小尺寸梯度聚合来降低启动开销的方
案，也包括优先传输紧急参数来增加计算和通信
重叠程度的方式。
3 参数同步模式优化
在大规模分布式机器学习训练场景中，计算
节点间需要频繁地进行参数同步，因此，参数同
步模式对整体训练性能具有重要的影响。本节将
从模型一致性协议和参数同步架构这两个方面详
细介绍对参数同步模式进行优化的相关工作。
3.1 模型一致性协议
在数据并行模式下，每个计算节点都需要保
存一份相同的模型副本，然后使用本地的训练数
据对模型副本进行更新。因此，在训练过程中，
不同计算节点所维护的模型副本会出现差异。为
了使得分布式训练能够取得与单机训练相同的效
果，需要保证这些模型副本的一致性。接下来，
介绍三种典型的模型一致性协议。
整体同步并行（Bulk Synchronous Parallel，
BSP）[18]是由著名的计算机科学家 Valiant 于 1990
年提出的，该协议已经被 TensorFlow、PyTorch
和 MXNet 等主流分布式机器学习框架所采用。
BSP 的特点是，在所有计算节点完成该轮参数同
步前，任何一个计算节点都不会进入下一轮计算
过程。具体来说，BSP 协议的同步过程分为以下
三个阶段（如图 6(a)所示，阴影部分为通信过程）：
1） 并发计算：各计算节点同时使用自己的
模型副本和本地的训练数据进行本地计算，获得
本地的模型更新；
2） 全局通信：不同计算节点间相互交换各
自的本地模型更新；
3） 屏障同步：为确保每个计算节点都已经
收到了来自其他计算节点的模型更新，计算节点
要进入屏障同步阶段，等待所有其他计算节点完
成计算和通信；
BSP 协议需要所有计算节点都进入屏障同步
后才能开始下一轮计算，在实际应用中存在慢机
问题。即，在计算节点的性能存在差异时，整体
的计算进度会被最慢的计算节点拖慢。
为解决 BSP 的慢机问题，Recht 等人提出了
异步并行（ASynchronous Parallel，ASP）[19]。ASP
的特点是，各计算节点间的计算进度是独立的：
快的计算节点在计算完一轮后只需要将本地模型
更新推送给全局模型，而无需等待慢的计算节点
完成便可以继续计算。相比于 BSP，ASP 协议的
同步过程没有屏障同步阶段（如图 6(b)所示）。值
得注意的是，ASP 协议虽然能够避免快的计算节
点被慢的计算节点拖慢，但是存在过时计算问题，
即，由于慢的计算节点基于陈旧的参数计算模型
更新，当其计算完成时，全局模型参数可能已经
被快的计算节点更新过多次，导致该模型更新过
时。如果计算节点间的性能差异非常大，模型可
能会无法收敛。由于 ASP 协议实现简单，主流的
分布式机器学习框架也均支持 ASP 协议。
延迟同步并行（Stale Synchronous Parallel，
SSP）[20]是一种介于 BSP 协议和 ASP 协议之间的
有界异步协议。其特点是，允许最快的计算节点
和最慢的计算节点间的迭代轮数之差不超过预先
设定的阈值 s：当迭代轮数之差不大于 s 时，计算
节点间无需等待；否则，最快的计算节点将被暂
停以等待最慢的计算节点。图 6(c)展示了 SSP 的
同步过程。BSP 和 ASP 可以被认为是 SSP 的特例：
当 s=0 时，SSP 特化为 BSP；当 s=∞时，SSP 特
化为ASP。SSP协议不仅在一定程度上解决了BSP
协议存在的慢机问题，而且有效地控制了模型更
新的陈旧程度。文献[20]和文献[21]对 SSP 在求解
凸优化问题时的收敛性进行了理论分析，给出了
SSP 与 BSP 之间的收敛性差异与 s 之间的关系。
因此在解决凸优化问题时，SSP 往往能够取得优
于 BSP 和 ASP 的表现。但该理论并不适用于非凸
优化问题。对于 DNN 等非凸优化问题，SSP 所导
王帅等：分布式机器学习系统网络性能优化研究进展 7
节点1 计算
节点2
节点3
计算
计算
计算
计算
计算
本轮屏障
 
节点1 计算
节点2
节点3
计算
计算
计算 计算
计算
ASP
 
节点1 计算
节点2
节点3
计算
计算
计算
计算
计算
计算
s=2，故对节点1生成屏障
(a) BSP (b) ASP (c) SSP
图 6 模型一致性协议
致的训练轮数的增加甚至超过了单轮训练速度的
提升，使得 SSP 的收敛速度甚至慢于 BSP[22]。另
外，随着 s 的增大，模型收敛时的准确性也会逐
渐下降[23]。目前，支持 SSP 协议的分布式机器学
习框架主要包括 SSPTable[20]和 Pettum[24]等。
如表 2 所示，不同的模型一致性协议在训练
速度和模型收敛速度方面各有优势。由于 SSP 和
ASP 缓解甚至完全消除了屏障同步，使得计算资
源能够被充分利用，从而提高了训练吞吐；但它
们往往需要更多轮数的训练才能收敛到和 BSP 相
同的水平。文献[19]证明了对于稀疏学习场景，即，
每次模型更新只会修改小部分参数，ASP 协议是
可以收敛的。文献[20]证明了对于 Lasso 回归这类
凸优化问题，SSP 协议是收敛的，并且实验结果
表明，相同时间内，使用 SSP 可以获得比 BSP 和
ASP 更好的训练效果。文献[21]的实验结果展示
了对于 DNN 这类非凸优化问题，BSP 效果最佳。
在实际应用中，BSP 协议由于收敛速度快，是目
前最常用的模型一致性协议。
表 2 模型一致性协议比较
协议 训练吞吐 收敛速度 最佳适用场景
BSP 低 快 同构计算；DNN 等非凸优化问题
ASP 高 慢 异构计算；稀疏学习
SSP 中 中
异构计算；Lasso 回归等凸优化问
题
3.2 参数同步架构
除模型一致性协议外，参数同步架构对大规
模分布式机器学习系统的性能也有至关重要的影
响。参数同步架构是指不同计算节点上的模型更
新进行汇总，并对模型副本进行更新的方式。按
每次模型更新是否推送给所有其他计算节点，参
数同步架构可以分为中心化架构和去中心化架
构；按模型更新是否在单一逻辑节点汇总，可以
分为集中式架构和分布式架构。一般来说，中心
化架构既可以是集中式架构，也可以是分布式架
构；而去中心化架构一定是分布式架构。目前常
用的参数同步架构大多为中心化架构，因此，除
特别说明外，下文所提到的参数同步架构均为中
心化架构。
3.2.1 扁平化的参数同步架构
参数服务器（Parameter server，PS）架构[25]
是一种最为典型的架构设计，目前已经被大部分
主 流 的 分 布 式 机 器 学 习 框 架 所 支 持 ， 包 括
TensorFlow、MXNet 和 Caffe 等。如图 7(a)所示，
PS 架构将参与计算的所有节点划分为两组，分别
是参数服务器组和工作者组。工作者组负责计算
模型更新，参数服务器组负责汇总模型更新并维
护全局模型。具体来说，PS 架构中，每次参数同
步过程可以分为以下 4 个步骤：
1） 参数拉取。每个工作者独立地从参数服
务器上拉取全局模型的参数以更新本地模型副
本；
2） 本地计算。不同工作者间并发地进行本
地计算：每个工作者基于本地模型副本和训练数
据计算模型更新，即，模型梯度；
3） 梯度推送。每个工作者独立地将所计算
的模型梯度推送给参数服务器；
4） 汇总更新。参数服务器对来自不同工作
者的模型梯度进行汇总，并将汇总后的结果用于
更新全局模型。
当参数服务器组由多台参数服务器共同组成
时，每台参数服务器只负责维护其中一部分参数
的梯度汇总更新。在这种分布式参数服务器架构
中，每台工作者需要与所有参数服务器建立连接，
并将每个模型梯度推送给相应的参数服务器。可
以看到，在参数服务器架构下，工作者之间以及
参数服务器之间是不需要通信的，只有工作者和
参数服务器之间存在模型数据的交互：工作者将
模型梯度推送给参数服务器，并从参数服务器上
拉取更新后的全局模型。值得注意的是，参数服
8 计 算 机 学 报
参数服
务器组
工作
者组
全局模型
本地副本
 
水平组0
水平组1
水平组2
垂直组0 垂直组1 垂直组2
第1步
第2步
第3步
(a) 参数服务器架构 (b) 环规约架构 (c) HiPS 架构 (d) 递归二分和倍增规约架构
图 7 中心化参数同步架构
务器和工作者只是逻辑功能的区分。在实际部署
中，同一个物理节点可以同时承担参数服务器和
工作者的职能，这样的架构也被称为全连接（Full 
Mesh）架构。
PS 架构具有诸多优势，如支持各种模型一致
性协议、弹性扩展较好、鲁棒性较强、容错性好
等。但该架构也存在一些问题，当参数服务器侧
总带宽小于工作者侧总带宽时，参数服务器很容
易成为网络瓶颈，导致整体的训练速度降低。
环规约（Ring Allreduce）架构是一种分布式
的架构设计，广泛应用于高性能计算领域。2017
年，百度硅谷人工智能实验室的一篇博客①提出将
Ring Allreduce 架构引入到分布式机器学习中来，
自此，该架构开始受到大规模分布式机器学习网
络研究者的关注。如图 7(b)所示，Ring Allreduce
架构将所有的计算节点以逻辑环的形式连接，每
个节点只与其前后两个邻居节点进行通信。假设
计算节点的数量为 n，Ring Allreduce 架构中，每
次参数同步过程可以分为以下四大步骤：
1） 本地计算：各计算节点基于本地的模型
副本和训练数据独立地计算模型梯度；
2） 分散-规约（scatter-reduce）：该阶段通信
过程分为 n-1 步，并且每个节点将模型梯度分为 n
段。在每一步中，每个节点向后一个邻居节点发
送一个梯度段，同时从前一个邻居节点接收一个
梯度段。待接收完成后，将接收到的梯度段内容
与本地相应的梯度段内容汇总，以用作下一步中
发送的内容。这样，任何一个梯度段在经过 n-1
步传输后，都汇总了全部 n 个计算节点上相应梯
度段的内容。即，最终，每个计算节点上都拥有
一个汇总了全部计算节点上相应梯度段内容的全
局梯度段；
3） 全收集（allgather）：该阶段与分散-规约
阶段相似：将汇总后的梯度段在环结构中依次传
 
① Bringing HPC Techniques to Deep Learning. https://andrew.gi
biansky.com/blog/machine-learning/baidu-allreduce/. 2017,02,21
输。不同的是，每个节点不再将接收到的梯度段
和本地内容汇总，而是直接用接收到的内容替换
掉本地内容。最终，在经过 n-1 步传输后，每个
计算节点上拥有了全部的更新后的梯度段；
4） 参数更新：各节点使用汇总后的梯度更
新本地模型副本。
相比于 PS 架构的“多对多”通信模式，Ring 
Allreduce 架构的“一对一”通信模式较为简单，
在实际应用中，不容易造成网络拥塞。实际上，
如果仅将带宽作为衡量通信成本的唯一因素，则
Ring Allreduce 架构是最优的[26]。但 Ring Allreduce
架构也存在一些问题，如难以支持 ASP 和 SSP 等
模型一致性协议以及容错性差等。Horovod[27]是
Ring Allreduce 架构的一种实现，支持以插件的形
式为 TensorFlow、PyTorch 和 MXNet 等多种机器
学习框架提供参数同步功能。Horovod 已经成为
实际部署中广泛使用的 Ring Allreduce 架构实现。
3.2.2 层次化的参数同步架构
PS 架构和 Ring Allreduce 架构虽然在参数规
约的具体实现方面存在差异，但是具有一个共同
的特点，即扁平化。当节点规模很大时，扁平化
的参数同步架构会带来诸多问题，如 Ring Allreduce 架构下会有每个梯度段过小以及传输次数过
多等问题；PS 架构会有跨机架流量大幅增多以及
incast 等等问题。层次化的参数同步架构可以有效
地缓解这些问题。
Geng 等人提出了层次化的参数同步框架
HiPS
[28]。HiPS 提出将计算节点按照层次化的方式
进行组织，在每一层次中将所有节点分组，同一
节点在不同层次中可以属于不同分组。不同层次
可以独立地使用不同的参数同步架构。这里以两
层 Ring Allreduce 为例进行说明。如图 7(c)所示，
将 n 个节点编排为水平方向和垂直方向各
n
组，
其中每组包括
n
个节点。第一阶段，每个水平
组内对全部参数进行分散-规约；第二阶段，每个
垂直组内对上一步规约后的结果继续进行分散-
王帅等：分布式机器学习系统网络性能优化研究进展 9
规约；第三阶段，每个垂直组内对上一步规约后
的结果进行全收集；第四阶段，每个水平组内将
上一步全收集的结果继续进行全收集。值得注意
的是，以上描述中，同一时刻仅有水平或垂直方
向的通信，因此，可以使用另一进程按照“垂直
组-水平组-水平组-垂直组”的顺序进行同步。由
于两个方向的并行同步，每个方向的同步数据量
可以减半。
HiPS 框架在索尼公司的工作[29]中也得到了
具体应用，并被命名为 2D-Torus 算法。HiPS 框架
的另一个实例化应用是 BML[30]。BML 以 BCube
网络拓扑为基础，通过将 HiPS 算法与其层次化的
互连结构相结合，实现高效的参数同步。
递 归 二 分 和 倍 增 （ Recursive Halving-Doubling， HD）[31]是一种典型的树型规约方
式（如图 7(d)所示）。同 Ring Allreduce 架构一样，
HD Allreduce 架构最早也是应用在高性能计算领
域，作为 MPI 在多进程之间实现高效数据归约的
方式。最近，HD Allreduce 架构也被引入到分布
式机器学习场景中来[32]。HD Allreduce 架构中，
每次参数同步的过程也可以分为分散-规约和全
收集这两个部分。不同于 Ring Allreduce 架构通过
环状连接实现相应功能的方式，HD Allreduce 架
构采用树状方式。具体来说，假设节点依次被编
号 1~n，并且任意两节点间的距离定义为其编号
的差值，则分散-规约过程可以分为
2
log n
步：第
1 步时，距离为 1 的两个节点，如 1 和 2、3 和 4
等等，相互交换一半各自的梯度，即，发送一半
梯度给对方，同时从对方接收另一半梯度，并汇
总接收到的梯度和本地相应的内容；第 2 步时，
距离为 2 的两个节点，如 1 和 3、2 和 4 等等，相
互交换一半各自节点上拥有的上一步中汇总后的
梯度。后续步骤依次将上一步汇总梯度中的一半
与距离倍增的节点进行交换。最终，每个节点上
都会拥有一个汇总了全部节点上相应梯度段内容
的全局梯度段。全收集过程相当于分散-规约的逆
过程，在此不再赘述。
HD Allreduce 架构中，每个计算节点一共需
要建立
2
log n
条连接，但每一步中仅有一条连接
会被使用。故其也是“一对一”的流量模式。相
比于 Ring Allreduce 需要 2*(n-1)次通信来说，HD 
Allreduce 仅需要
2 2*log n
步，这在节点规模很大
时，可以显著降低通信的次数。然而，HD Allreduce
架构中，不同步骤的通信需要在不同的节点间建
立连接，而非像 Ring Allreduce 架构一样在不同的
步骤中使用相同连接，这就极大地增加了产生拥
塞的可能性。
3.2.3 去中心化的参数同步架构
近来，去中心化的参数同步架构也开始受到
研究者的关注[33-34]。在去中心化的参数同步架构
下，每个节点只与自己的邻居交换信息，当交换
完成后，便可以继续下一轮计算过程。具体来说，
首先，每个节点使用本地模型和训练数据计算本
地梯度；然后，每个节点和其邻居交换参数信息，
并将本地参数和邻居的参数信息汇总后更新本地
模型；最后，使用本地梯度进一步更新本地模型。
可以看到，任一节点都会利用到邻居的计算结果，
同时其计算结果也都会被其邻居所利用。最重要
的是，通过邻居之间的多次传递，任一节点的计
算结果最终都会传播到其他所有节点上。因此，
在一些文献中该算法也被叫做 gossip 算法[35]。相
比于中心化架构，去中心化架构的主要优势在于
有效地减少了通信流量。因此，当网络带宽较低
时，采用去中心化架构的训练速度要快于中心化
架构。然而，目前去中心化架构的应用场景仍然
较少，主要应用于凸优化领域[36-37]。去中心化参
数同步架构与异步训练具有相同的本质，即，某
个计算节点的模型更新在多轮以后才会被传递到
另一个计算节点。因此，在实际训练中，去中心
化的参数同步架构仍会影响非凸优化问题的收敛
速度。
3.2.4 参数同步架构小结
参数同步架构对通信的流量模式、通信次数、
通信量等具有决定性影响。
表 3 参数同步架构性能比较
架构 通信次数 理论同步时间
PS 2
2 1   2
n
a
nB


Ring Allreduce 2(n-1)
2 1   2 ( 1)
n
a n
nB

 
HD Allreduce
2log2 n  
2
2 1
2 log
n
a n
nB


2D-Torus
4*( 1) n  2 1   2 ( 1)
n
a n
nB

 
BML
4*( 1) n  2 1   4
n
a
nB


表 3 总结了不同参数同步架构的特点，其中
10 计 算 机 学 报
n 为计算节点数量，参数总量为单位 1，B 为计算
节点的总通信带宽，a 为两节点间进行一次通信
的时延开销。这些架构的通信量均为 2*(n-1)/n，
其中 BML 假设为 2 层结构。可以看到，PS 架构
中，每个节点的通信次数为常数，Ring Allreduce
的通信次数最多，层次化的参数同步架构可以大
幅减少通信次数。从理论同步时间上来看，这些
参数同步架构的差别主要在于时延项的不同。其
中，Ring Allreduce 架构由于需要串行地通信
2*(n-1)次，其时延项占比较大。这也从理论上解
释了为何 Ring Allreduce 架构在节点规模很大时，
性能会有所下降。
4 通信效率优化
在实际部署中，通信的性能和效率也会对参
数同步过程产生重要的影响。即使采用相同的参
数同步模式，不同的通信方式也会对整体的训练
性能造成很大的差异。因此，为了提高分布式机
器学习训练时的通信效率，研究人员在以下方面
进行了深入研究。
4.1 传输协议和通信库
传统的 TCP 协议虽然在广域网中得到了广泛
的应用，但是在数据中心场景中却无法满足日益
增长的流量需求。为弥补 TCP 在数据中心环境下
的不足，研究人员提出了一些 TCP 的变体[38-42]。
其中，Alizadeh 等人提出的 DCTCP[39]针对数据中
心场景对 TCP 进行了优化，大大降低了传输时延。
传统的 TCP 协议将丢包作为拥塞信号，当发送端
检测到丢包时，才会减小发送窗口。这就使得大
量的数据包在交换机队列中被缓存，从而导致排
队时延大幅增加。DCTCP 通过交换机队列长度判
断当前网络的拥塞程度，当交换机队列长度达到
一定阈值时，交换机便会在数据包中加上 ECN 标
记。接收端收到 ECN 标记的数据包后，在回传给
发送端的 ACK 包中同样加以标记。发送端根据收
到的 ACK 包中带有 ECN 标记的比例来调节发送
窗口，从而实现更加及时且更加准确的拥塞控制。
由于交换机队列使用率始终维持在一个比较低的
水平，因此，数据包所经历的排队时延将会大大
降低。更重要的是，由于交换机队列长度被控制
在一个较低的水平，DCTCP 能够在一定程度上缓
解 PS 架构下产生的 incast 问题。然而，当发送端
数量很多时，DCTCP 仍无法避免交换机缓冲区溢
出的问题，导致尾部流完成时间增加。
Xia 等人提出的 MLT 传输协议[43]是为分布式
机器学习场景设计的专用传输协议。作者发现，
机器学习算法具有有限的丢包容忍性，即，当网
络随机丢包率小于一定阈值时，模型的收敛性基
本不受影响。基于这一观察，作者设计了一种新
的具备有限丢包容忍性的传输协议 MLT。不同于
TCP 的完全可靠传输和 UDP 的不可靠传输，MLT
提供了部分可靠传输特性，即保证预先设定的比
例的数据包会被可靠地交付给接收端，而只为其
他的数据包提供尽力而为的传输。当丢包发生时，
采取如下方式处理：对可以被快速检测到的丢包
仍会重传；对超时重传这类耗时较长才能被检测
到的丢包，且丢包比例未超过预设阈值时，则直
接丢弃。作者的观察结果显示，对 RNN 等模型，
丢包容忍阈值一般在 10%~35%之间。因此，通过
避免大量的超时重传，MLT 可以有效地降低尾部
流完成时间。仿真结果表明，相比于 TCP，基于
MLT 的 分 布 式 机 器 学 习 训 练 速 度 可 提 升
10%~120%。需要强调的是，虽然 MLT 在所测试
的模型和数据集上可以提高模型的训练速度，但
是其在不同模型和数据集上的普适性尚未被理论
证明。因此，其他模型基于 MLT 协议训练时的收
敛性仍无法保证。
除对 TCP 协议本身进行优化外，近年来，远
程直接内存访问（Remote Direct Memory Access，
RDMA）技术也被引入到分布式机器学习训练中
来，并成为大规模分布式机器学习训练的常见配
置。RDMA 是一种硬件技术，它可以在无需 CPU
干预的情况下直接访问远程主机内存。具体来说，
相比于 TCP，RDMA 具有以下优点：（1）零拷贝。
数据直接从网卡传输到应用程序的缓冲区，避免
了内存拷贝和系统调用开销；（2）内核旁路。通
过将网络协议栈卸载到硬件中，绕过内核，RDMA
大大减少了总体延迟。（3）无需 CPU 参与。RDMA
可以访问远程内存，而无需占用 CPU 时间，从而
节省了更多的 CPU 资源用于模型训练。因此，与
现有的 TCP 方案相比，RDMA 可以实现更高的吞
吐量和更低的延迟。目前，业界存在三种具体的
RDMA 技术标准，分别是 InfiniBand（IB）、RDMA 
over Converged Ethernet （ RoCE ） 和 internet 
Wide-area RDMA Protocol（iWARP）。这三种技术
都需要硬件支持，其中，IB 网络使用专用的网络
协议栈，从硬件级别保证可靠传输，因此部署成
王帅等：分布式机器学习系统网络性能优化研究进展 11
本非常高，广泛应用于高性能计算领域；RoCE
和 iWARP 则是基于以太网的 RDMA 技术，可以
使用普通的以太网交换机，但需要使用专用的
RDMA 网卡。在实际应用中，受到底层 TCP 协议
的限制，iWARP 的性能不及 RoCE①，因此，为了
兼顾成本和性能，数据中心内使用的 RDMA 技术
大多为 RoCE 协议。
目前，分布式机器学习框架中使用 RDMA 通
信的方式可以分为两种，IB Verbs 和 GPU Direct 
RDMA（GDR）。对于 GPU 通信，前者需要先将
数据从 GPU 显存拷贝到主机内存，然后再使用
RDMA 技术传输到对端主机内存，对端 GPU 则
需要将数据从主机内存拷贝到 GPU 显存。相比之
下，GDR 则为 GPU 提供了直接访问对端 GPU 显
存的能力，从而减少了 GPU 通信过程中数据拷贝
的次数，使得通信延迟进一步降低。Yi 等人的工
作[44]表明，在包含 16 块 GPU 卡的实验床上，相
比于TensorFlow中原生的基于TCP的通信方式和
Yahoo 公司提出的基于 IB Verbs 的通信方式②，
GDR 可以分别将端到端训练速度提高 2.43 倍和
1.21 倍。
1 2 3 4
4 3 2 1
4 3 2 1
等待 4, 3 2 1
前向计算
反向计算
梯度传输
梯度先聚
合再传输 时间
图 8 MG-WFBP 聚合算法示意图
Shi 等人发现参数同步过程中小尺寸梯度的
频繁传输会导致传输效率的降低，因此需要尽量
避免传输小尺寸梯度[45]。一般来说，当传输大小
为 M 的数据，其通信开销为
T M a bM ( )  
(4)
其中，a 为时延项，b 为传输单位大小数据的
传输时间。可以发现，M 越小，时延项所占比例
越大，即通信效率越低。如图 8 所示，将多个小
尺寸梯度聚合后再进行传输虽然会提高通信效
率，但也会造成梯度更新后需要经过更长的时间
才会被传输，从而降低计算和通信的重叠度。作
者基于以上通信开销模型对分布式机器学习训练
 
① RoCE vs. iWARP Competitive Analysis. https://www.mellano
x.com/related-docs/whitepapers/WP_RoCE_vs_iWARP.pdf. 2017,2
② TensorFlow on Spark. https://github.com/yahoo/TensorFlowOn
Spark. 2017,2,5 
的单次迭代过程进行建模，并提出 MG-WFBP 算
法来求解最优聚合方案。实验结果表明，在 8 节
点环境下，相比于无需等待的反向传播 WFBP 和
反向计算完成后将所有梯度聚合再传输这两种方
案，MG-WFBP 可将端到端训练速度提升 1.2~1.36
倍。
NVLink③是 GPU 厂商 Nvidia 推出的一种总线
及其通信协议，它能够在多 GPU 之间以及 GPU
与 CPU 之间实现高速互联。2016 年，Nvidia 发布
了首款搭载 NVLink 的 P100 GPU，双向带宽达到
160GB/s，相当于 PCIe Gen3 * 16 带宽的 5 倍。2017
年，Nvidia 发布的 V100 GPU 上所搭载的 NVLink 
2.0 可以将 GPU 双向带宽进一步提升至 300GB/s。
2020 年，Nvidia 发布的 A100 GPU 上所搭载的
NVLink 3.0 将通道数从 6 提升至 12，总带宽也达
到 600GB/s，几乎相当于 PCIe Gen4 * 16 带宽的
10 倍。NVLink 虽然可以提供更高的互联速度，
却需要 GPU 和 CPU 等硬件的支持。目前，仅 IBM 
Power 系列的部分 CPU 支持 NVLink。
Nvidia 还开发了集合通信库 Nvidia Collective 
Communications Library（NCCL）④，可以在多 GPU
以及多主机间实现高效的集合通信。NCCL 高度
优化和兼容了 MPI，提供了 allgather、reduce、
broadcast 等集合通信原语，并且具有 GPU 互联拓
扑感知能力，可以在 PCIe、NVLink、RDMA 上
实现较高的通信速度。NCCL 1.0 版本仅支持单机
多卡通信，GPU 卡之间可以通过 PCIe、NVLink
通信；NCCL 2.0 版本扩展了多机多卡通信能力，
不同主机间可以通过 TCP 或 RDMA 来通信。相
比于 MPI，NCCL 是为 Nvidia GPU 硬件量身打造
的集合通信库，对数据搬移、任务下发、内存访
问等细节均有很好的处理，因此可以大幅提高通
信性能。Nvidia 官方提供的数据⑤显示，在 32 节
点规模下，使用 NCCL 时的端到端训练性能是
MPI 的 1.95 倍。
在使用环规约架构时，需要保证所有节点均
按照相同的顺序对模型参数进行同步，否则可能
造成死锁。Horovod[27]采用“主从模式”来保证参
数同步顺序的一致性：如图 9(a)所示，从进程将
 
③ NVLink and NVSwitch. https://www.nvidia.com/en-us/data-cen
ter/nvlink/. 2020
④ Nvidia NCCL. https://developer.nvidia.com/nccl. 2020
⑤ NCCL 2.0 report. https://on-demand.gputechconf.com/gtc/2017/
presentation/s7155-jeaugey-nccl.pdf. 2017
12 计 算 机 学 报
本地待同步的参数 ID 集合发送给主进程，主进程
收集到所有从进程发来的参数同步请求时，先筛
选出所有请求的交集，然后将交集中包含的参数
ID 广播给所有从进程。这样，每个从进程按照只
需要按照收到的交集中的参数顺序进行同步就可
以保证全局一致性。以上过程分两个段：请求收
集阶段使用 MPI_Gather 集合通信原语，交集广播
阶段使用 MPI_Bcast 集合通信原语。随着节点规
模的增大，这种“两阶段协商”机制的性能逐渐
下降。为了改善这一问题，Laanait 等人提出了“单
阶段协商”机制 -- bitAllReduce[46]。如图 9(b)所
示，该机制首先将每个节点上的待同步参数 ID 转
化为位图，然后利用 MPI_Allreduce 集合通信原语
在所有的从进程间对不同节点上的位图进行规
约，从而做到仅需一步便可完成参数同步顺序协
调。作者在 1024 节点规模下测试发现，使用
bitAllReduce 时的扩展效率约为 NCCL 的 2 倍。
目前 bitAllReduce 机制已经被集成到 Horovod 通
信库中。
主进程0
请求:{ , , }
汇总 
 , , 
 , , 
响应:{ , }
取交集
从进程1
请求:{ , , }
响应:{ , }
MPI_Gather
MPI_Bcast
(a) “两阶段协商”机制
主进程0
请求:{ , , }
响应:{ , }
转化
从进程1
请求:{ , , }
响应:{ , }
MPI_Allreduce
位图: [1 1 1 0]
转化
位图: [1 0 1 1]
位图: [1 0 1 0] 位图: [1 0 1 0]
转化 转化
(b) “单阶段协商”机制
图 9 参数同步顺序协商
虽然 GPU 卡间可以存在多种通信通道，如
PCIe、NVLink 以及 RDMA 等，但是 NCCL 在任
意两块 GPU 卡间仅会采用一种最高效的通信通
道，无法充分利用所有的通道。除无法利用异构
的通信通道外，任务调度也会造成同一任务使用
的 GPU 之间使用异构的通信通道。
针对这一问题，Wang 等人提出了可以充分利
用 GPU 间所有的同构/异构数据传输通道的数据
聚合方案 Blink[47]。首先，Blink 打破了业界广泛
采用的环规约架构，使用生成树来构建参数同步
架构，从而避免环规约架构中存在的某一条低带
宽链路限制整体通信速度的问题；其次，针对异
构通信通道，Blink 可以根据通道的带宽差异，对
不同通信通道上的数据传输比例进行相应的划
分，从而达到均衡利用所有通信通道的目的。文
中对使用 NVLink 和 PCIe 进行异构连接的 GPU
间通信吞吐进行测试，由于 NVLink 未能在 GPU
间形成环式连接，NCCL 2.0 仅能使用 PCIe 通道，
而 Blink 则可以充分利用所有通道，从而将吞吐从
4.8GB/s 提升至 26.4GB/s。
4.2 网内聚合
随着可编程交换机的发展，研究人员开始探
索如何将参数汇总功能从服务器转移到交换机，
从而大幅提高参数汇总节点的进出口带宽，或者
达到减少网络传输数据量的目的[48-51]。虽然网内
聚合在概念上比较直观，但是在真实的可编程交
换机上实现网内聚合功能却面临着以下挑战：
1） 交换机的计算能力有限。参数聚合功能
的数学本质是计算若干数据的平均值。机器学习
模型参数是以浮点型存储的，然而，目前可编程
交换机仅支持整型运算和逻辑运算。此外，可编
程交换机也不支持乘除计算。因此，可编程交换
机无法直接计算模型参数的平均值。
2） 交换机的存储空间有限。一方面，从发
展趋势上来看，机器学习模型的参数量在不断增
大。例如，当采用数据并行方式时，BERT 模型
的参数同步数据量高达 1.3GB①。相比之下，可编
程交换机的存储空间仅数十 MB [52]。另一方面，
存储需求还会随着计算节点的增多而增大。因此，
可编程交换机无法将来自众多计算节点的模型参
数先存储到本地再进行聚合。
3） 无可靠传输保证。从协议分层的角度看，
可编程交换机不具备四层功能，即，无法保证数
据包的可靠传输。当发生丢包时，不会触发重传
机制，使得接收端陷入等待接收数据状态无法继
续下一轮计算。
Sapio 等人在 2017 年发表的一篇短文[48]分析
了实现网内聚合面临的挑战，并针对 MapReduce
应用实现了概念验证原型系统。继而，在 2019 年
提出了 SwitchML 架构[50]，通过对交换机侧和端
侧进行协同设计，将参数汇总过程分解，由端侧
辅助完成复杂操作，在一定程度上解决了以上问
题。
 
① BertLarge 分布式训练（流水并行）. https://www.alibabacloud.c
om/help/zh/doc-detail/194800.htm. 2021,1,19
王帅等：分布式机器学习系统网络性能优化研究进展 13
首先，为了保证可编程交换机的线速转发能
力，SwitchML 在端侧将浮点数转换为整型数。考
虑到数据类型转换导致的精度损失，端侧先将原
始模型参数进行适当伸缩，再将伸缩后的数据转
换为整型数传输到交换机上。这样，当收到交换
机汇总的结果后，只需要再通过逆操作便可以得
到精度更高的汇总结果；其次，针对可编程交换
机存储空间不足的问题，SwitchML 采用了基于聚
合器池的流式处理协议。交换机侧具有由若干（设
为 k）整形聚合器构成的聚合器池，端侧将模型参
数切分成固定大小，并指定该参数切片使用的聚
合器。端侧采用自同步的方式发送切片，即，初
始时发送 k 个切片，随后每收到一个聚合结果便
发送一个新的切片，从而保证交换机侧处理的数
据量不会过载。图 10 展示了 SwitchML 对两个计
算节点执行网内聚合的过程，其中每个节点各 5
个参数切片，并保存在
G0
或
G1
中，参数汇总后
的结果保存到
A0
或
A1
中。最后，SwitchML 设计
了重传机制，以避免陷入死锁状态。若端侧在一
定时间内未收到聚合结果，则判定为发生了丢包，
触发端侧重传。交换机侧收到重传报文后，若此
前未收到该报文，则将该报文所携带的数据交给
指定的聚合器，否则，则判定为聚合结果丢失，
需要重传该结果。因此，聚合结果需要在交换机
侧保留一定时间。SwitchML 采用“阴影复制”的
方法，在交换机侧提供两个互为主从的聚合器池，
端侧交替使用两者。由于自同步机制只有在收到
聚合结果后，才会发送新的切片来使用该聚合器，
因此，当交换机的池 A 中聚合器 i 聚合完来自所
有端侧的切片时，可以获知由池 B 中聚合器 i 聚
合的结果已经被所有的端侧接收到，池 B 的聚合
结果可以被释放掉以接收新的切片。
实验结果表明，在 16 节点计算规模下，相比
于 NCCL+RDMA（或 NCCL+TCP），SwitchML
可将 DNN 模型的端到端训练速度提升高达 2.2 倍
（ 或 5.5 倍 ）。 具 体 到 通 信 性 能 ， 相 比 于
NCCL+RDMA（或 NCCL+TCP），SwitchML 可将
通信性能提高 2.9 倍（或 9.1 倍）。虽然 SwitchML
在简单场景下取得了较大的性能提升，但如何将
其扩展到多机架、多任务场景，仍需要更进一步
的设计，并且部署成本很高。最重要的是，该方
案不支持 SSP 和 ASP 等异步训练方式。
Mellanox 公司提出的可扩展分层聚合和缩减
节点0
 
 
节点1
 
 
+ = 交换机
 
 
 
 
交换机
 
说
明
时间
 
 
 
图 10 SwitchML 网内聚合过程
协议（Scalable Hierarchical Aggregation Protocol，
SHARP）技术①通过将集合通信需要的计算操作从
CPU 卸载至交换机改进了 MPI 操作的性能。
SHARP 技术使用专用的片上浮点运算器（FPU）
完成计算卸载。相比于 SHARPv1，SHARPv2 采
用流式聚合方式，提高了计算处理能力。SHARP
技术是基于 IB 网络设计的，因此，流控等机制均
依靠底层网络，无法直接工作在以太网。此外，
将计算操作固化到 ASIC 芯片虽然能够提高处理
速度，但升级迭代也因此受到了限制。
除了将参数聚合的计算操作卸载到交换机
外，还可以提高参数服务器的网络通信能力。
Parameter Hub[53]使用 PBox 架构为机架规模的参
数服务器提供高吞吐、低时延的通信性能，同时
兼顾集群的部署成本。PBox 的目标是匹配 IO 带
宽和内存带宽。如作者使用的原型系统中，参数
服务器的内存双向带宽为 120GB/s，因此可以使
用 10 张 56Gbps 的网卡将该参数服务器与交换机
进行连接，从而平衡 IO 带宽和内存带宽。除了硬
件匹配外，还需要应用能够充分利用底层硬件的
通信能力。为此，PBox 采用 chunk-to-core 的映射
机制（见图 11），将数据块与网卡和 CPU 核绑定，
任一网卡的通信上下文（如 RDMA 通信的
completion queue 和 queue pair）只会被一个 CPU
核读写，从而保证数据局部性，避免跨核流量降
低通信性能。PBox 还针对更大规模部署进行了扩
展设计。当在多个机架上执行训练任务时，PBox
采用分层规约算法。该算法分为以下三步：
 
① Mellanox SHARP. https://cn.mellanox.com/products/sharp. 2020
14 计 算 机 学 报
1） 每个机架内的 PBox 将该机架内所有工
作者的梯度进行汇总；
2） PBox 节点将部分汇总结果在机架间进
行汇总，并计算全局梯度更新；
3） 每个机架内的 PBox 将该全局梯度更新
作用于模型参数，并将更新后的模型参数广播给
该机架内的所有工作者。
作者对部署成本建模发现，当 ToR 交换机的
超额订购比为 2：1 时，相比于分布式参数服务器
架构，单位成本下 Parameter Hub 可以将训练吞吐
提高 26%。同 SwitchML 一样，Parameter Hub 没
有针对 SSP 和 ASP 等异步通信方式进行完善的设
计。虽然 Parameter Hub 需要尽量保证不同工作者
发送的梯度尽量同时到达 PBox 节点，以充分利用
缓存性能，但是 Parameter Hub 中 PBox 服务器有
更大的内存来存储收到的梯度。因此，相比于
SwitchML 和 SHARP 来说，Parameter Hub 扩展到
SSP 和 ASP 等异步训练场景的难度相对较低。
参数1
CPU核
NUMA0
CPU核
NUMA1
网络接口
参数0 参数0 参数1
网络接口 网络接口
交换机
节点0 节点1
PBox
图 11 PBox 架构
4.3 模型压缩
模型压缩是指将梯度或参数压缩后再进行传
输的方法。该方法虽然可以大大减少传输的数据
量，但数据精度在压缩后也会降低，即数据压缩
是有损的。这就导致模型在训练时的收敛性会受
到影响，因此，模型压缩方法是以更多的训练轮
数为代价来提高每一轮的训练速度的。图 12 展示
了两种常见的模型压缩方式：量化（Quantization）
和稀疏化（Sparsification）。
一般来说，模型参数或梯度采用 32 比特的全
精度来表示每一个元素，而量化方法则通过使用
更少的比特数来表示每一个元素，而传输的元素
个数仍与原始数据相同。例如，对一个形状为
[1024, 1024, 1024]的参数，使用全精度表示时，其
大小为 4GBytes，而使用 16 比特的半精度来表示，
其大小可以被压缩一半。文献[54]证明了在假设优
化问题具有凸且稀疏的性质时，这种量化压缩方
式是可以保证模型收敛的。
…
原始参数 …
每个元素占
用32比特
(a) 原始参数格式
…
量化 …
每个元素占
用4比特
(b) 量化参数格式
…
稀疏化 …
每个元素占
用32比特
(c) 稀疏化参数格式
图 12 模型压缩方式对比
1-Bit SGD[55]将所有的梯度均进行了1比特量
化，大大减少了参数同步过程传输的数据量。为
了减少量化引入的误差，受 Delta-Sigma 调制的启
发，1-Bit SGD 在将梯度进行量化时会保存此次量
化引入的误差，并将该误差和下一轮计算得到的
梯度相加后再将新的梯度进行量化：
1
( ) ( ( ) ( )) q t t t G Q G i i i   
   
(5)
1
( ) ( ) ( ( )) t t t q
i i i    G Q G    
(6)
1
1
1
( )
n
t t t q
i
i
G
n
   


  
(7)
其中，
Q(·)
为对工作者节点 i 上的梯度 G 进
行量化的函数，
t 
为 t 时刻的模型参数。在经过
量化压缩后，
( ) q t Gi 
的大小只有
( )t Gi 
的 1/32，
因此可以将网络传输的数据量大大减小。实验结
果表明，在经典的 Switchboard 语音识别系统上，
1-Bit SGD 可将训练速度提升 10 倍。值得说明的
是，由于误差

会在随后的计算过程中不断更新
并对梯度进行补偿，直至其累计量足够大时才会
被聚合，因此，1-Bit SGD 也可以被认为是另一种
形式的异步训练方式。1-Bit SGD 在其他场景下是
否能够收敛仍未有理论证明。
Alistarh 等人分析了通信数据压缩程度和模
王帅等：分布式机器学习系统网络性能优化研究进展 15
型 收 敛 速 度 之 间 的 关 系 ， 并 基 于 此 提 出 了
Quantized SGD (QSGD) [56]。QSGD 不只包括一种
量化方式，而是一系列量化压缩算法。使用该算
法可以权衡每次迭代所传输数据的比特数。QSGD
的量化算法如下：给定非 0 参数向量

，对其中
的任一元素
i
，其量化之后的值为
Q s s i i i ( ) || || sgn( ) ( , )       2
(8)
其中，
Qs (·)
表示量化级别为 s 的压缩函数，
sgn(·)
为符号函数，
 i( , )s
为随机变量，其定义
如 下 。 假 设
0  l s
， 且 l 为 整 数 ， 那 么
2   i
/ || || [ / ,( 1) / ]   l s l s 。 [ / ,( 1) / ] l s l s 
就
是
2
/ || ||   i
的量化区间，其具体取值如下：
2
/ , 1 ( , )
( , )
( 1) ,
i
i
l s p s
s
l s

  

 
 

 
概率
否则
(9)
这里，
i
表示
i
的长度，
p a s as l ( , )   , 
a0,1。由此量化算法压缩之后的结果满足无
偏性和方差有界性，并且压缩后参数向量的期望
满足非 0 元素的数量不超过
s s n ( ) 
)，其中，n
为参数向量中元素的总数量。
作者在 ImageNet[57]和 CIFAR-10[58]等数据集
上评估了 QSGD 对图片分类模型的加速效果。实
验结果表明，在使用 16 块 GPU 训练 AlexNet 模
型时，使用 QSGD 可以减少 75%的通信时间，同
时模型收敛时间可降低 60%。此外，作者也对语
音识别场景下 QSGD 的加速效果进行了评测，模
型收敛速度可提高 2.7 倍。和 1-Bit SGD 相比，
QSGD 的性能更优，尤其是在训练 ResNet 和
Inception 等卷积层较多的神经网络时，QSGD 的
优势更加明显。此外，相比于 1-Bit SGD，QSGD
无需为每个元素维护量化误差，更加节省内存空
间。
不同于量化方式，稀疏化则是通过过滤掉不
重要的参数或梯度来减少传输的元素个数，未被
过滤掉的元素与原始数据保持一致。一般来说，
随着模型趋于收敛，会有越来越多的梯度值将逐
渐趋于 0，这些足够小的梯度值对模型的收敛速
度影响远不及大的梯度值。因此，不必每一轮训
练都传输这些小梯度。仍以上述形状为[1024, 
1024, 1024]的参数为例，若其中有 90%的元素值
足够小，则采用稀疏化方式可以将传输数据量压
缩至 0.4GBytes。由此可见，相比于量化方式，稀
疏化方式更加激进。
直观上，为了将梯度进行稀疏化处理，只需
要设定一个合适的阈值

：若某一维度的梯度值
小于

，则将其舍弃；否则，按原值处理[59]。这
种简单截断的方法虽然简单，但梯度值较小也有
可能是由于当前样本对该维度的训练不足导致
的，将其简单截断可能导致该维度特征的丢失。
Langford 等人在 2009 年提出了截断梯度法
（Truncated Gradient）[60]，以改进简单截断的不
足。截断梯度法在截断梯度时更加缓和，除要与
阈值

比较外，还要另一个阈值

比较：若梯度
值与 0 的差值大于

与 0 的差值，则以梯度值与

的差值来更新，否则以 0 来更新。这样，截断
梯度法使用两个参数来控制稀疏程度：两个值越
大，模型越稀疏。简单截断法和截断梯度法的区
别如图 13 所示：
 
- 
 
 
 
 
- 
 
 
- 
 
(a) 简单截断法 (b) 截断梯度法
图 13 固定阈值稀疏化方法
这种采用固定阈值的稀疏化方法的缺点在于
需要选取合适的阈值。然而，由于模型和训练环
境都具有多样性，在实际训练时，很难针对特定
的模型和训练环境选取到合适的阈值。除固定阈
值外，还可以采用动态阈值的方式[61-65]，即 Top-k
稀疏算法。
Dreden 等人依据预先设定的传输比例来确定
正阈值和负阈值[64]。在每一轮迭代中，大于正阈
值或者小于负阈值的梯度将会被传输，未被传输
的梯度将会在随后的迭代中逐渐积累直至超过阈
值被传输。该算法可以保证压缩比率恒定。Aji
等人对该算法进行了改进[65]，用一个绝对值阈值
来代替正负两个阈值：如果某个梯度的绝对值大
于该阈值，则被传输；否则，将被丢弃。此外，
针对不同梯度的尺度不同的问题，Aji 等人也提出
使用层归一化（layer normalization）方式来归一
化不同的梯度，从而优化全局阈值的效果。
除梯度可被压缩外，参数也可以被压缩。然
而，Top-k 稀疏算法中，不同工作者独立地选取需
要传输的梯度，容易导致不同工作者选取的梯度
16 计 算 机 学 报
差别很大。这样，在每一轮迭代中，被更新的参
数数量几乎与工作者数量成正比。假如有 m 个工
作者，每个工作者独立地选取 k 个梯度，且任意
两个工作者选取的梯度均完全不同，则会有 k*m
个参数会被更新。由此可见，在大规模训练时，
几乎所有的参数都会被更新，并被传输到工作者
上以供下一轮计算使用。由此可知，参数并未被
有效地压缩。Shi 等人提出 gTop-k 算法[66]利用树
结构在梯度聚合过程中逐轮稀疏化，从而将参数
服 务 器 到 工 作 者 的 流 量 从 O(k*m) 降低到
O(k* logm
)。
量化和稀疏化方式也可以结合起来进一步压
缩传输数据量。SBC（Sparse Binary Compression）
算法[67]首先利用传输比例 p 筛选出绝对值较大的
梯度，然后将这些梯度分为正梯度值和负梯度值，
并分别求得正平均值


和负平均值

 。若正平
均值


大于负平均值

 ，则将所有负梯度值置
为 0，所有正梯度值置为


；否则，将所有正梯
度值置为 0，所有负梯度值置为

 。这样，只需
要传输一个梯度值和非 0 梯度的索引即可。
总的来说，模型压缩，尤其是稀疏化，可以
大幅减少分布式机器学习训练过程的通信数据
量。以 SBC 算法[67]为例，梯度稀疏化的压缩率可
达到 1/2363。然而，对于任意的机器学习模型，
这些压缩算法是否会影响模型收敛性尚需充分评
估[54]。
4.4 通信调度
分布式机器学习系统中每轮迭代的时间除与
计算耗费时间和通信耗费时间相关外，还受计算
和通信之间的重叠程度影响。从计算的角度看，
机器学习模型具有层次化结构：在每轮迭代中，
前向计算过程按照从输入层到输出层的方向逐层
计算，并将第i层的计算结果作为第i+1层的输入；
反向计算过程则按照从输出层到输入层的方向逐
层计算，并基于前向计算的中间结果和第 i+1 层
产生的模型误差来计算第 i 层参数所对应的梯度。
从通信的角度看，现有的分布式机器学习框架大
多采用无需等待的反向计算方式（ Wait-Free 
Back-Propagation， WFBP）[68]，即，第 i 层梯度
计算完成后可以立即被同步，而无需等待所有梯
度计算完。综合计算过程和通信过程来看，参数
同步的顺序和前向计算消耗的参数顺序恰好相
反，比如，前向计算首先消耗第 1 层参数，而该
参数在反向计算的最后才会被同步，由此导致下
一轮迭代时必须要等待所有参数都同步完才能开
始新的计算过程。近年来，为了优化模型计算和
网络通信之间的重叠程度，一些相关工作[8-9][69-71]
提出对参数同步过程进行更加合理的调度，从而
有效地利用网络资源。
2019 年，Hashemi 等人 Error! Reference source not 
found.通过大量实验注意到 TensorFlow 等主流的分
布式机器学习框架在同步参数时未考虑计算和通
信之间的重叠，参数的传输顺序具有随机性。随
机的参数传输顺序为迭代时间带来了不确定性，
不但容易导致计算资源的浪费，而且不同工作者
获取到参数的顺序不一致，也会导致落后者效应，
即，获取到参数的顺序最差的工作者成为落后者，
其他工作者不得不等待其计算完成后才能开始新
一轮计算。
进一步地，Hashemi 等人通过理论分析发现
参数调度问题为 NP 难问题，并提出两种启发式
算法—时间无关的通信调度算法 TIC 和时间相关
的通信调度算法 TAC，并统称为 TicTac。TIC 算
法以机器学习模型对应的计算图为唯一输入，通
过对计算图进行遍历，得到各通信操作符所依赖
的通信次数，然后按照依赖的通信次数由少到多
的顺序依次降低通信操作符的优先级。TAC 算法
的输入除计算图外，还包括操作符消耗的时间。
基于更丰富的信息，TAC 算法可以进一步优化参
数传输顺序。如图 14，A 和 B 两个通信操作符所
依赖的通信次数相同，TIC 算法将会随机分配优
先级，但 TAC 算法则会通过比较计算操作符 1 与
接收操作符 B 的重叠程度和计算操作符 2 与接收
操作符 A 的重叠程度来决定 A 和 B 的优先级。
接收操作符 A
接收操作符 B
计算操作符1
计算操作符2
计算操作符3
图 14 TIC 和 TAC 区别
TicTac 为每个通信操作符分配一个唯一的优
先级编号 p，表示发送端在执行该通信操作符前
需要先完成 p 个通信操作符。实验结果表明，对
于 ResNet-101 等典型的 DNN 模型，TicTac 可以
将训练和推理速度分别提升 20%和 37%。然而，
相比于 TIC 算法，TAC 算法并未表现出明显的性
能优势，这说明对于大多数机器学习模型来说，
计算图已经可以提供足够的信息来实现接近最优
王帅等：分布式机器学习系统网络性能优化研究进展 17
的参数调度。此外，TicTac 所使用的优先级调度
方式为非抢占式调度，即，高优先级参数未被发
送前，低优先级参数即使已经就绪，也无法被发
送，反而造成网络资源的浪费。
与 TicTac 同时，Jayarajan 等人也注意到参数
同步的顺序不但要考虑梯度生成的顺序，更要考
虑参数消耗的顺序，并提出了基于优先级的参数
调度算法 P3Error! Reference source not found.。P3 认为层粒
度的参数同步过于激进，会导致资源利用率低下。
图 15(a)展示了一个具有 3 层参数的 DNN 模型的
参数同步过程。层粒度的参数同步使得在参数同
步时很难同时利用所有资源，反观使用细粒度的
参数切片①时，在第 3 秒开始便可以同时利用计算
资源和双向带宽资源，从而降低参数同步时间。
基于参数切片的思想，P3 为每个参数切片分
配一个优先级编号：被前向计算消耗的越早，优
先级越高，并且同一层的参数切片具有相同优先
级。待传输的参数切片在计算完成后进入优先级
队列，P3 从中选取优先级最高的参数切片进行同
步。由于传输的粒度从层级别细化为切片级别，
这样，即使更高优先级的参数切片进入优先级队
列，最多也只需要等待一个切片的传输时间，从
而既保证了优先传输高优先级参数，又可以在无
高优先级参数时传输低优先级参数，即抢占式调
度。
 
 
 
0 1 2 3 4 5 6 7 8 9 10 11
发送梯度 参数更新 接收参数
时间
(a) 层粒度
 
 
 
0 1 2 3 4 5 6 7 8 9 10 11
发送梯度 参数更新 接收参数
时间
 
 
(b) 切片粒度
图 15 参数同步耗时
在 4 节点集群上的实验结果表明，对于
VGG-19 模型，P3 通过对参数切片进行调度可将
 
①
Lij
表示第 i 层参数对应的第 j 个切片。
训练速度提升 49%，并且当互联带宽从 30Gbps
降低到 15Gbps 后，P3 和标准分布式机器学习训
练之间的性能差异进一步拉大到 66%。需要说明
的是，切片的大小对于训练速度有着重要的影响。
随着切片粒度变小，高优先级参数可以更及时地
被传输，网络资源也可以被更加充分地利用。然
而，过小的参数切片会导致参数的切分和聚合开
销提高，甚至消除参数切片带来的性能增益。作
者通过实验得到最优切片大小的经验值为 50000。
P3 采用“停止-等待”的方式实现抢占式调度：
每次只发送最高优先级的参数切片，便停止发送
直至收到该次传输的确认才会继续发送下一个参
数切片。该方式虽然可以保证高优先级参数切片
及时被传输，但是会造成网络带宽的浪费。针对
这一问题，ByteScheduler[8]提出了基于 credit 的抢
占式调度。类似滑动窗口，credit 允许发送多个参
数切片而无需等待收到其确认，同时又对未被确
认的参数切片数量进行了限制。例如，有 4 个参
数切片
p1 ～ p4
，其优先级依次升高。当
p1
在被
传输时，
p2 、 p3、 p4
依次就绪。若 credit 为 2，
则当
p2
就绪时，即使
p1
仍在传输，
p2
也可以立
刻被传输，即，传输顺序为
p1
-> p2
->
p4
-> p3
；
若 credit 为 1，即“停止-等待”方式，则只有当
1 p
传输结束时才会传输其他参数，因此，传输顺序
为
p1
->
p4
-> p3
-> p2
。由此可见，较大的 credit
虽然可以提高带宽利用率，但会降低高优先级抢
占的及时性。ByteScheduler 将 credit 和切片大小
作为两个系统参数，并使用贝叶斯优化算法对其
进行优化，以获得最优的训练性能。此外，
ByteScheduler 是第一项同时兼容多种机器学习框
架和参数同步架构以及传输协议的工作。实验结
果表明，在相同环境条件下，相比于 P3，
ByteScheduler 对 VGG-16 等模型的训练性能提升
可多达 43%。
以上通信调度方案均工作在端主机侧，通过
控制分布式机器学习应用向主机内的通信协议栈
传递的参数顺序来实现通信调度。然而，在分布
式参数服务器场景下，这些调度方案无法对不同
参数服务器上的参数传输进行调度。假设有 2 个
不同优先级的参数，且位于不同的参数服务器节
点，这些端侧调度方案由于只能控制每个节点内
的参数传输顺序，因此无法保证工作者尽早地接
收到高优先级参数。为了保证高优先级参数在网
18 计 算 机 学 报
络中可以被优先传输，Wang 等人提出了全网级别
的参数调度方案 Geryon[9]。传统的分布式机器学
习框架在任意两个节点间仅建立一条连接，
Geryon 打破了这一限制，提出在任意两节点间可
以建立多条连接，并为这些连接分配不同的优先
级，然后使用不同优先级的连接传输参数。具体
来说，Geryon 首先根据参数所处的位置为该参数
分配一个紧急度。假设该模型共 L 层，则第 l 层
参数的紧急度为 1-l/L。然后，通过紧急度阈值为
参数分配相应的优先级。比如，使用两个优先级
连接，且紧急度阈值为 0.6，则紧急度为 0.7 的参
数将会被分配高优先级，即，该参数/梯度的传输
均使用高优先级连接。
Geryon 是第一个将参数调度转化为流调度的
工作。流调度方案具有以下特点：
1） 抢占式。大多数网络设备已支持严格优
先级调度（strict priority），既可以提高带宽利用
率（work-conservation），又可以保证高优先级参
数可以被及时转发；
2） 细粒度。流调度通常以数据包（1KB）
作为调度的最小粒度，远小于参数切片大小
（200KB）。更重要的是，将参数拆分为多个数据
包或是将多个数据包还原为参数的操作可以通过
LSO/LRO 等硬件卸载技术完成，大大提高了网络
吞吐；
3） 全网规模。优先级信息以标签的形式插
入到每个数据包的包头，因此，无论是在端侧还
是在交换机侧，均可以基于优先级标签对数据包
进行调度，从而避免了端侧调度方案中参数进入
网络中后无法被调度的问题。
在 8 台 GPU 服务器上的实验结果表明，在
10G 网络下，Geryon 可将分布式训练的扩展效率
提高至 95%。相比之下，标准的 TensorFlow 分布
式训练仅有不足 40%的扩展效率。另外，与端侧
调度方案的对比实验展示了 Geryon 的性能几乎
不受切片大小的影响，而端侧调度方案的性能则
随着切片尺寸的减小逐渐降低。
近年来，异构计算在机器学习领域已变得非
常普遍。然而，异构硬件计算平台为分布式机器
学习集群的迭代升级带来了成本问题。以 GPU 为
例，对大规模分布式机器学习集群来说，整体升
级成本非常大，因此当新一代 GPU 面世时，往往
通过在原有集群的基础上增设部分新一代 GPU
进行系统扩容，从而导致具有不同计算性能的多
代 GPU 共存的现象在大规模机器学习集群中十
分常见。当使用不同性能的计算设备来进行分布
式机器学习训练时，低性能的节点成为整体训练
的瓶颈。针对这一问题，Wang 等人提出了面向迭
代同步应用的计算高效的流调度方案 CEFS[71]。
一般来说，传统的流调度方案通常以最小化
平均流完成时间或者最大化有效完成流数量为优
化目标。然而，在以分布式机器学习训练为代表
的迭代同步应用中，优化这些指标并不意味着训
练性能的提升。CEFS 指出，对于分布式机器学习
训练，流调度的目标应该是最小化计算资源的闲
置时间，从而最大化整体的训练性能。如图 16 所
示，一方面，CEFS 优先调度紧急参数，使每个工
作者都可以尽早地开始计算；另一方面，CEFS
优先调度低性能节点，使低性能节点的计算被阻
塞的时间最小化，从而有效缓解其对整体性能的
影响。这样，对于任一参数，其优先级除受其紧
急度影响外，还取决于其目的节点的性能。CEFS
采用贝叶斯优化算法为每个参数分配优先级，并
采用二维保序规则来过滤掉不符合要求的分配结
果。二维保序规则是指：1）对同一个节点，分配
给较高紧急度参数的优先级不低于较低紧急度参
数的优先级；2）对同一个参数，分配给较低性能
节点的优先级不低于较高性能节点的优先级。
3 2 1
3 2 1
1 2 3
1 2 3 3 2 1
3 2 1
3 2 1
3 2 1
0 时间
节点 0 
带宽
节点 1 
i 第i层前向计算
i 第i层反向计算
i 第i层参数传输
i 第i层梯度传输
5 10 15 20
(a) 无调度
1 2 3
1 2 3
3 2
3 2 1
1
1 2 3 3 2 1
1 2 3 3 2 1
带宽
节点 0 
节点 1 
(b) 参数调度
1 1 2 2 3 3 3 3 2 2 1 1
1 2 3 3 2 1
1 2 3 3 2 1
节点 0 
节点 1 
带宽
(c)参数调度和节点调度
图 16 CEFS 调度效果示意图
在 16 台异构 GPU 服务器上的实验结果表明，
相比于标准的 TensorFlow 和 ByteScheduler，CEFS
王帅等：分布式机器学习系统网络性能优化研究进展 19
可分别将训练吞吐提升多达 253%和 47%。CEFS
之所以能够达到更高的性能，主要在于其能够更
好地保证参数传输顺序。例如，训练相同的模型，
CEFS 的参数传输乱序度①为 57，而 TensorFlow 和
ByteScheduler 则分别为 300 和 126。此外，对于
VGG-19 等经典 DNN 模型，贝叶斯优化算法在经
过 10 轮左右迭代后，可以使模型的训练速度收
敛。
不同于其他优化方案，虽然参数调度优化可
以在既不影响模型收敛性，又无需额外的硬件升
级的前提下加快分布式机器学习训练速度，但需
要强调的是，以上通过参数调度来加速分布式机
器学习训练的方案均对模型的计算/通信比有一
定的要求。当计算/通信比过高或者过低时，即使
对参数进行有效的调度，也无法显著提高计算和
通信的重叠程度，因此无法改善分布式训练性能。
例如，在网络互联带宽为 1Gbps 时，P3 和标准的
MXNet 分布式几乎拥有相同的性能，因为此时通
信时间已远大于计算时间。由此可见，通信调度
优化的应用场景还是受到一定限制的。
5 并行方式优化
如上文所述，数据并行和模型并行是两种经
典的分布式机器学习训练方式。对于数据并行来
说，通信开销主要来自不同计算节点间的参数同
步；对于模型并行来说，当某个计算节点的输入
来自另一个计算节点的输出时，便会产生通信开
销。当模型参数量小于中间计算结果的数据量时，
数据并行带来的通信开销较小；反之，模型并行
的通信开销更小。然而，对模型整体使用某一种
并行方式，可能无法达到最优的训练性能。因此，
一些工作[68-74]提出使用混合并行、流水并行等方
式，通过细粒度的并行优化来提升分布式训练性
能。
卷积神经网络主要由卷积层和全连接层构
成，但这两种功能层却呈现不同的特点。卷积层
参数仅占全部参数的很少部分（通常低于 10%），
却需要大量的计算（通常高于 90%），而全连接层
则恰好相反。基于这一观察，Stanza[72]提出将卷
积层和全连接层的训练解偶（见图 17）：使用大
 
① CEFS提出乱序度来衡量参数到达工作者的顺序和最优顺序之
间的差异。乱序度为违反计算图依赖关系的参数之间的距离之和。
部分计算节点以数据并行的方式训练卷积层，剩
余的小部分计算节点以数据并行的方式训练全连
接层。这样使得原本产生大部分通信流量的全连
接层参数同步仅发生在少数的计算节点间，有效
地减少了参数同步带来的通信流量。同时，卷积
层和全连接层构成模型并行。一般来说，最后的
卷积层输出数据量较小，在卷积层计算节点和全
连接层计算节点间传输中间计算结果时，即使使
用多对一/一对多的通信方式，引入的通信流量也
比较小。此外，卷积层参数同步可以和全连接层
参数同步重叠起来，进一步缩短参数同步时间。
为了确定负责卷积层和全连接层的计算节点数
量，Stanza 建立了性能模型，以最大化训练吞吐
为优化目标，对计算节点分配问题进行求解。
作者使用 Nvidia Tesla V100 GPU 在 10G 带宽
环境下测试发现，随着计算节点数量的增加，
Stanza 相比于参数服务器架构的性能提升越来越
显著。在 10 节点规模下，Stanza 相对于参数服务
器架构的性能提升高达 13.9 倍。
训练数据
卷积层训
练节点
全连接层
训练节点
激活值 梯度值
… …
图 17 Stanza 训练示意图
Geng 等人从同步开销、GPU 利用率、负载均
衡、落后者消除以及 I/O 扩展性等 5 个方面对数
据并行和模型并行进行比较[73]，得出了以下结论：
在负载均衡和 I/O 扩展性方面，数据并行更具优
势；在其他方面，模型并行则更有优势。具体来
说，由于数据并行下每个工作者的计算量基本相
同，因此负载均衡性能较好。相比之下，模型并
行划分的多个子模型之间的计算量差异则相对较
大，导致不同工作者的负载相对不均。此外，数
据并行可以减小每个工作者节点的 I/O 数据量，
因此具有较好的 I/O 扩展性。基于该结论，作者
提出了混合并行训练架构 Hove。Hove 首先将计
算节点分组，组内使用模型并行，组间使用数据
并行，从而保证 I/O 扩展性；然后采用“先组内
汇总，后组间同步”的方式提高参数同步效率；
最后，在负载不均时，通过自动调节机制对模型
进行动态划分，以消除落后者。
PipeDream[74]在混合并行的基础上进一步引
20 计 算 机 学 报
入流水方式来优化分布式机器学习训练性能。混
合并行中使用模型并行划分计算任务虽然比较简
单，但却面临一个重大挑战：机器学习训练为双
向过程，反向计算需要基于前向计算的中间结果
来更新参数。因此，当某个子模型完成前向计算
后，该计算节点需要保存计算结果以保证反向计
算的正常执行。如下图所示，这种简单的模型并
行方式会极大地浪费计算资源。为解决这一问题，
PipeDream 提出了新的任务调度算法 1F1B。如图
18 所示，1F1B 以流水线的方式处理多个训练批
次：当某个计算节点完成其所分配的子模型的计
算，便将计算结果传给后一计算节点，同时该计
算节点开始对下一个批次执行前向计算。当最后
一个节点完成前向计算后立刻对该批次执行反向
计算，并在反向计算完成后把梯度计算结果传给
前一节点继续执行反向计算过程，同时，该计算
节点开始对下一个批次执行前向计算。这样，当
进入稳定状态时，每个节点均以前向计算和反向
计算交替的方式进行计算，并且不存在计算资源
浪费的情况。
基于模型 profiling 结果，PipeDream 的模型
划分算法可以输出如何划分模型、各子模型需要
的计算节点数量以及使得流水线饱和的最优并发
批次数量。然而，流水并行带来的新问题是模型
更新作用的参数版本和前向计算使用的参数版本
不一致。为便于说明，定义
t l
为被 t 个批次更新
过的子模型 l 的参数。如图 18 所示，如果在每个
节点上只维护一个参数版本，那么对于工作者节
点 0 来说，批次 5 的前向计算使用的参数为
1 0 ，
但是，被批次 5 所计算的梯度作用的参数却是
4 0 。
PipeDream 使用参数暂存（weight stashing）技术
来避免这一问题。参数暂存会在每个工作者节点
上为每一个活跃的批次维护一个参数版本，前向
计算时使用最新版本的参数，反向计算时使用的
参数版本和前向计算保持一致。如图 18，工作者
节点 0 会将批次 5 的梯度作用在
1 0
上，而非
4 0 。
虽然不同计算节点使用的参数版本仍不一致，但
实验结果展示了参数暂存技术可以有效地保证模
型收敛性。
为了避免模型并行导致的计算资源浪费问
题，GPipe[75]也提出了微批次（micro-batch）流水
并行策略。如图 19 所示，在前向计算时，GPipe
首先将每个批次划分为多个微批次，然后以流水
线方式处理多个微批次。当所有微批次完成反向
计算时，不同微批次产生的梯度会被汇总，并用
来更新模型。由于所有流水线操作均发生在一次
迭代内，因此，不同于 PipeDream，GPipe 不存在
参数版本过时的问题。但相比于 PipeDream，GPipe
的计算过程仍存在“气泡”，即计算资源闲置，的
情况。文献[74]的对比实验结果表明，PipeDream
的训练性能是 GPipe 等简单流水并行方式的 1.7
倍。
1 2 3 4
1 2 3 4
1 2 3 1
1 1 2 2
1
3 3 4 4 5 5 6 6 7
4 2 3 5 4 6 5 7 6
3 6 4 7 5 8
节点0 1 5 2 6 3 7 4 8 5
节点1
节点2
节点3
时间
i 第i个批次的前向计算 i 第i个批次的后向计算 闲置
 
 
 
 
 
 
 
 
 
 
 
 
 
 
每个节点维护
 多个参数版本 
 
 
图 18 PipeDream 流水并行和参数暂存示意图
 
 
 
节点0
节点1
节点2
节点3 
 
 
 
 
 更
新
参
数
时间
 子模型i处理微批
次j的前向计算 子模型i处理微批
次j的后向计算 闲置
图 19 GPipe 流水并行示意图
训练
数据 本地模型参数
协调器 训练引擎
8.存储参数 6-1.获取本地参数
本地模型参数
协调器 训练引擎
6-1.获取本地参数
6-2.获取远程参数
7.读取训
练数据
节点
0
节
点
1
映射表
令牌桶
令牌生成器 令牌分发器
2-1.添加令牌 3.获取令牌
5-1.分发令牌
1.请求令牌 5-2.发送参数请求
2-2.记录“节
点-令牌”映射 4.获取映射信息
令牌
服
务
器
图 20 Fela 架构
Fela[76]采用更细粒度的灵活并行策略。更细
粒度是指将模型逐层划分，并对不同层使用不同
的 batch size。这是因为作者发现，不同层在计算
时达到饱和所需的 batch size 不同。当使用统一的
batch size 训练不同层时，由于显存限制，无法使
用足够大的batch size以使所有层均能达到计算饱
和状态。因此，Fela 将模型训练拆分成多个子任
务，并采用令牌分发的方式协调不同子任务（见
图 20）。Fela 将服务器分为令牌服务器和工作者两
个角色。不同于参数服务器，令牌服务器只负责
令牌的生成和分发，并不维护模型参数。在每次
迭代开始时，工作者首先向令牌服务器发送请求，
王帅等：分布式机器学习系统网络性能优化研究进展 21
由令牌服务器返回令牌；然后工作者根据令牌信
息训练相应的子模型，并将新的参数保存在本地；
最后，工作者向令牌服务器发送任务完成通知，
并请求新的令牌。当令牌服务器收到其他工作者
的任务完成通知后，将通知所有工作者互相同步
子模型参数。此外，需要强调的是，由于模型计
算是逐层进行的，因此令牌服务器在生成令牌时
需要等前层的计算结果就绪时才会生成后层的令
牌。
6 网络拓扑优化
除以上优化方案外，分布式机器学习系统领
域的研究者也对分布式训练集群所使用的底层物
理网络拓扑提出了优化方案。
Wang 等人通过理论分析和仿真实验发现基
于 Fat-Tree 网络拓扑（见图 21）同步参数时，同
步性能要低于BCube拓扑[77]。如图22所示，BCube
是以服务器为中心的拓扑结构，每个服务器节点
均有多个网卡，因此 BCube 提高了每个节点的互
联带宽。这样，即使对于同样的参数同步架构，
相比于 Fat-Tree，基于 BCube 进行参数同步也可
以有效减少理论参数同步时间。以 HiPS 算法为
例，若使用 k 级 BCube 拓扑，即每个服务器节点
上有 k 块网卡并分别连接到不同的交换机上，其
理论参数同步时间仅为 Fat-Tree 拓扑的 1/k。仿真
实验结果表明，当在 Fat-Tree 拓扑中使用 RDMA
协议同步参数时，存在流量负载不均衡，PFC 暂
停帧蔓延等问题。需要注意的是，全局参数同步
时间主要取决于所有节点间最晚完成的参数传
输，因此流量负载不均会导致不同节点间参数传
输时间出现很大的差异，从而增大全局参数同步
时间。除性能优势外，BCube 也具有成本优势：
构建相同规模的集群，BCube 所需要的交换机数
量仅为 Fat-Tree 的 k/5。基于以上分析，作者认为
分布式机器学习训练集群的底层物理网络拓扑应
使 用 BCube ， 而 非 传 统 数 据 中 心 中 常 用 的
Fat-Tree。
阿里巴巴自建的分布式机器学习训练集群
EFLOPS[32]也注意到在传统的分布式训练方案中，
服务器上一般会配置多个 GPU 但只配备一张网
卡，导致多 GPU 访问网络时，唯一的网卡成为限
制系统性能的瓶颈。为了提高服务器的带宽，
EFLOPS 为每个 GPU 配备专用的网卡。即使同一
节点内的 GPU 之间通信也会被导出到节点外，从
而绕过 PCIe 带宽瓶颈。
边缘层
交换机
汇聚层
交换机
核心层 交换机
图 21 Fat-Tree 拓扑
第0层
交换机
第1层
交换机
图 22 BCube 拓扑
顶层交换机
底层交换机
计算节点
计算节点
图 23 BiGraph 拓扑
基于多网卡服务器结构，EFLOPS 提出了
BiGraph 网络拓扑。如图 23 所示，BiGraph 将网
络分为上下两层，两部分之间通过 Clos 架构互连。
顶层交换机和底层交换机均可以连接服务器，且
服务器与同一交换机之间存在多条链路。这样，
任一顶层服务器和底层服务器通信均只需要 3
跳，且最短路径具有唯一性。这一特性为应用控
制流量路径提供了支持，即，当通信的两端确定
时，流量路径也是确定的。因此，利用该特性，
EFLOPS 提出了带编号映射的递归二分和倍增
（Halving-Doubling with Rank-Mapping，HDRM）
算法。HDRM 算法在 HD Allreduce 的基础上，通
过将逻辑连接和物理链路进行一一映射，使得通
信仅发生在不同部分的服务器之间，且在每一步
中每条链路上仅存在一条数据流，以避免不同连
接之间的链路争用问题，从而彻底解决网络拥塞
问题。作者通过对比实验展示了，相比于 HD 
Allreduce ， HDRM 算 法 可 将 通 信 延 迟 降 低
40%-50%，同时将网络吞吐提高一倍。
此外，Liu 等人提出的 PSNet 拓扑[78]也通过
22 计 算 机 学 报
增加参数服务器和交换机互连链路的数量来提高
AWGR
AWGR 阵列波导光栅路由器
交换机
电路
光路
计算节点
子模块
图 24 Lotus 拓扑
网络带宽和系统容错性。Lu 等人针对分布式机器
学习集群的流量特点为其专门设计了基于阵列波
导光栅路由器（Arrayed Waveguide Grating Router, 
AWGR）的网络互联拓扑 Lotus[79]。如图 24 所示，
Lotus 在每个子模块内部采用 Clos 架构提供丰富
的链路资源，同时在不同子模块间也创建多条直
连链路，从而缩短路径长度，并且提高对分带宽。
阵列波导光栅路由器为无源光交换设备，并且可
以在任意端口间实现快速无阻塞的通信，因此，
使用阵列波导光栅路由器不但可以提高网络设备
的转发能力，并且可以降低网络设备功耗。
总的来说，针对分布式机器学习训练的场景，
对底层物理网络拓扑进行适配，可以极大地提高
通信能力，减少数据流之间的带宽资源竞争，从
而大幅降低通信时间。然而，该类方案涉及整个
集群互联方式的大规模调整，灵活性较低，因此
比较适用于构建新的训练集群。此外，现有网络
拓扑优化工作均面向单训练任务专用集群进行设
计，很难扩展到多训练任务共享集群。以 BiGraph
拓扑为例，当某个训练任务仅占用部分计算节点
时，这些计算节点可能无法构成小规模的 BiGraph
拓扑，导致 HDRM 算法无法将逻辑连接和物理链
路进行一一映射，从而使得网络拓扑优化无法达
到预期效果。
7 优化方案总结与对比
表 4 综合对比了近年来研究人员所提出的分
布式机器学习系统网络性能优化研究相关工作。
对比的主要指标包括优化机制、训练加速效果、
节点扩展性、对模型收敛性的影响以及是否需要
更换硬件设备或者互联方式等。这些工作从多个
层面对分布式机器学习系统的网络性能进行优
化，不同机制之间各有优劣。
从训练加速效果来看，ASP 这一模型一致性
协议将通信开销从模型训练的核心路径上移除，
使得网络通信不会阻塞训练过程，加速效果非常
好；模型压缩或并行方式优化等方案，有效地减
少了各计算节点通信的数据量，而网内聚合方案
则逐跳减少了网络中的流量，因此，这些方案具
有非常好的加速效果；传输协议和通信库优化类
方案提高了点到点通信性能，网络拓扑优化类方
案提高了通信节点之间的互联带宽，这些方案的
加速效果也很好；虽然不同参数同步架构的理论
参数同步时间之间的差距主要来自于时延开销，
但在实际中，负载均衡、多流竞争等都会影响不
同参数同步架构的实际参数同步时间，总体来说，
参数同步架构类方案的加速效果不如前面几种方
王帅等：分布式机器学习系统网络性能优化研究进展 23
案；通信调度类方案的加速效果与训练模型的通
表 4 分布式机器学习网络性能优化方案的综合比较
方案类型 优化机制 训练加速效果 节点扩展性 是否影响模型收敛性
是否需要更换硬件
设备或互联方式
相关工作
模型一致性
通过松弛模型一致性来缓
解通信对计算造成的阻塞
高 高
BSP 无影响；
SSP 和 ASP 有影响
否
[18][19][20][21]
[23][54][80][81]
参数同步架
构
通过设计合理的参数同步
架构以避免流量热点，甚至
减少网络流量
中 中
中心化架构无影响；
去中心化架构有影响
否
[25][27][28][29]
[30][31][32][33]
[34][35]
传输协议和
通信库
从底层硬件、拥塞控制协
议、应用层逻辑等多个层面
优化单次传输的性能
高 中 否
RDMA 和 NVLink
等方案需要；
其他方案不需要
[27][39][43][44]
[45][46][47]
网内聚合
借助可编程交换机或专用
交换机将参数汇总操作卸
载到网络中，从而减少网络
流量
高 低 否 是
[48][49][50][51]
[53]
模型压缩
通过对参数/梯度进行稀疏
化或量化处理来大幅减小
通信数据量
高 高 是 否
[54][55][56][60]
[61][62][63][64]
[65][66][67]
通信调度
通过优先传输对计算阻塞
程度较高的通信操作，以达
到隐藏通信开销的目的
低 低 否 否
[6][9]Error! 
Reference 
source not 
found.Error! 
Reference 
source not 
found.
[71]
并行方式
扩大并行策略的探索空间，
利用更优的策略来提升并
行效率，降低通信需求
高 中 否 否
[72][73][74][75]
[76][82] [83]
网络拓扑
通过设计对分带宽更高的
网络互联拓扑，提高节点间
通信能力，减少通信耗时
高 中 否 是 [77][78][79][84]
信/计算比高度相关，相比其他方案来说，加速效果
比较有限。
从节点扩展性来看，网内聚合类方案受限于交
换机硬件计算能力和存储空间限制，通常应用于单
机架规模的训练集群，扩展性较差；随着计算节点
数量的增多，通信/计算比越来越高，导致通信调度
类方案在节点数量较多时的扩展性较差；由于交换
机端口数量、布线难度等因素的限制，底层物理网
络拓扑的规模往往不能无限增大，如 BCube 适用于
集装箱规模的数据中心，故网络拓扑类方案的扩展
性一般；虽然传输协议和通信库类方案可将通信性
24 计 算 机 学 报
能提高数倍，暂缓网络瓶颈出现的时间，但随着节
点数量的增多，通信操作又将成为系统瓶颈，故该
类方案的扩展性也一般；并行方式优化类方案的模
型并行粒度不能无限切分，因此在节点规模很大
时，仍会出现大量节点使用数据并行的情况，并且
对大量节点求解最优并行方式的算法复杂度也非
常高，如 PipeDream 的求解时间与计算节点数的二
次方和模型层数的三次方成正比，以上因素导致并
行方式优化类方案的扩展性也一般；节点规模很大
时，环规约架构的通信时间被时延开销所主导，参
数服务器架构的连接数量也会大大增加，从而导致
传输性能的降低，故参数同步架构类方案的扩展性
一般；模型一致性协议从核心路径上移除了通信，
模型压缩类方案可将通信量降低数十乃至上百倍，
因此这两类方案的扩展性较高。
从对模型收敛性的影响来看，模型压缩会导致
参数同步时信息量的丢失，从而影响模型收敛性；
SSP和ASP以及去中心化参数同步架构引入了陈旧
参数对全局模型的更新，也会对模型收敛性产生一
定影响；其他类方案不涉及通信内容的改变，故不
影响模型收敛性；
从对硬件的依赖性来看，RDMA 和 NVLink 需
要专用的硬件设备，故依赖于硬件设备的更新升
级；网内聚合类方案依赖于可编程交换机或专用交
换机来实现在网络内部对参数进行聚合的目的，故
该类方案也需要底层硬件设备的支持；网络拓扑类
方案涉及对整个集群互联方式的修改，比较适用于
新训练集群的搭建，在现有集群上的部署难度较
大；其他类方案均为软件层方案，对底层硬件环境
无特殊要求，因此部署难度较低，具有非常好的通
用性。
2016 年，Google 提出了一种分布式机器学习
新形式—联邦学习[85]。 本质上，联邦学习是一种
加密的分布式机器学习框架，允许各参与方在不共
享本地数据的条件下与其他各方共建模型。不同于
传统分布式机器学习，联邦学习面临四个新的问
题：客户端中数据非独立同分布问题、差分隐私问
题、通信开销问题和客户端无状态问题。本文仅关
注联邦学习中的通信开销问题。在联邦学习中，通
信开销远大于计算开销，这主要是由于客户端与中
央服务器之间的网络带宽有限，且连接质量较差，
同时不同客户端的连接质量参差不齐造成的。
虽然联邦学习是一种分布式机器学习框架，但
有些针对传统分布式机器学习的网络性能优化方
案却不适用于联邦学习。例如，客户端可能通过无
线方式接入网络，故无法对这些客户端之间的互联
方式进行改善；一般来说，联邦学习的网络瓶颈点
基本在客户端侧，因此，网内聚合的方式不能解决
联邦学习场景下的网络传输痛点；联邦学习中各客
户端均要使用本地数据进行训练，并且不会将本地
数据传输给其他客户端，因此联邦学习只能使用数
据并行方式，无法通过并行方式优化的方式来提高
训练速度。
联邦学习场景下的网络性能优化主要依赖对
通信内容的压缩来实现。一般来说，客户端上行链
路的带宽比下行链路带宽更小，因此，一些工作
[85-87]最早尝试通过多种梯度压缩方式，如量化、稀
疏化、下采样、矩阵分解、计数草图（count sketch）
和周期平均等，来减小客户端的上行通信压力。随
后，一些工作[88]通过压缩参数的方式降低下行通信
成本。文献[88]采用 Federated Dropout 的方式对神
经元进行随机丢弃，这样客户端可以只训练一个更
小的子模型，从而既减小了中央服务器到客户端的
通信数据量，又能更加高效地完成本地计算。
虽然模型压缩会减缓模型的收敛速度，但受限
于网络连接质量，联邦学习不得不通过压缩通信内
容的方式来降低通信成本，提高训练速度。相比之
下，传统分布式机器学习训练集群的互联带宽非常
高，并且连接可靠性极高，因此，模型压缩在传统
分布式机器学习训练中往往作为可选方案，需要充
分权衡收敛性和训练速度来决定是否需要对模型
进行压缩，以及使用何种压缩方式和压缩比例。
8 研究趋势展望
分布式机器学习系统性能优化作为分布式机
器学习领域最为热门的研究方向之一，正在吸引越
来越多学术界和工业界研究人员的关注。由于分布
式机器学习系统网络性能优化研究与工业界结合
紧密，具有重要的实践价值，可以预计在未来数年
内相关研究还将持续成为焦点。
当前，国内学术界和工业界关于分布式机器学
习系统网络性能优化的研究基本与国际水平处于
并跑状态。因此，在国家大力发展新基建的背景下，
加强分布式机器学习系统网络性能优化研究，不但
能够为人工智能的发展提供内生动力，并且可以为
依托人工智能实现外部赋能创造条件，对于推动传
王帅等：分布式机器学习系统网络性能优化研究进展 25
统行业信息化、数字化、智能化转型升级具有非常
重要的意义。
从网络通信的角度看，我们认为未来的分布式
机器学习系统性能优化研究主要包括以下四个方
向：
（1）模型的高质量压缩。分布式机器学习训
练的通信数据量对通信耗时具有决定性影响。从机
器学习算法的发展趋势来看，越来越大的机器学习
模型已经成为必然[16][89]。因此，如何对训练超大模
型时的通信数据进行高质量的压缩，既能大幅降低
通信数据量，又不会造成训练信息的大量丢失，是
未来缓解甚至彻底消除网络瓶颈的重要方向。当
前，模型压缩程度仍然受到相关理论发展的限制，
通信数据的压缩是以更多的通信次数为代价的。除
相关压缩理论的突破外，未来可能的发展方向还包
括细粒度的模型压缩方式，如不同层乃至不同算子
采用不同的压缩方式、压缩比例，不同训练轮数采
用不同的压缩方式、压缩比例，从而避免最差压缩
比例限制整体的压缩效果。另一个可能的方向是综
合考虑时空相关性的模型压缩方式，当前的压缩算
法大多将每个参数值作为单独个体来处理，部分算
法引入时间序列相关性以将相邻两轮训练间的结
果相关联，从而降低随时间累积的压缩误差。然而，
参数张量的空间相关性尚未得到充分重视。视频压
缩领域中，基于时空相关性的视频帧间压缩方法已
得到广泛应用。因此，模型压缩可以借鉴视频压缩
领域的相关经验，综合考虑参数张量的时空相关性
对模型参数采取进一步的有效压缩。
（2）并行方式优化。除压缩通信内容外，改
进多节点之间的并行训练方式也是降低通信开销
的重要途径。分布式机器学习训练通过将训练数
据、训练模型分布到多个计算节点来达到并行训练
的目的。即使对同一训练模型和相同训练数据而
言，不同的并行方式也会产生完全不同的流量模式
和通信数据量。现有方案大多在训练数据、模型不
同层等维度对训练任务进行并行化分解，最近一些
工作又引入了流水并行来提高计算资源利用率。但
这些方案仍远未成熟。对流水并行来说，由于层间
计算依赖关系的存在，这些方案或者无法完全消除
“气泡”，或者需要占用大量显存来存储多个模型
版本。如何提高流水并行的效率，同时最小化硬件
资源占用，对流水并行的应用前景至关重要。对模
型并行来说，更细粒度的操作符拆分，使其能够并
行化计算，从而提高单个操作符的执行速度上限，
也是未来值得探索的重要方向。另外，在大规模分
布式机器学习训练场景下，如何快速求解最优并行
化方式，将大量的计算节点合理地进行编排，也是
该类方案将来能否得到广泛应用的重要基础。
（3）多任务场景下的网络资源复用。现有的
网络性能优化方案仍主要针对单任务场景而设计，
对多任务之间的联合优化方案仍有待研究。但在实
际训练场景中，计算设备往往被单一训练任务所独
占，但网络设备却被很多训练任务共享，导致不同
训练任务由于彼此竞争网络资源造成性能的互相
影响。对于分布式机器学习训练任务来说，流量具
有明显的周期特征，即，从宏观结构来看，平均流
量并不高，但缩放到毫秒粒度，则会出现链路利用
率在满载和空载之间频繁切换的情况。当多个训练
任务同时使用网络资源时，所有训练任务的通信时
间都会被拉长，导致训练速度的下降。因此，未来
一个可能的发展方向便是通过使不同训练任务分
时复用网络资源，尽量减小每个训练任务所花费的
通信时间，从而提升整体的训练速度。
（4）专用网络设备和架构。网络硬件技术的
提升，对于分布式机器学习系统性能的提升具有显
著的效果。当前，分布式机器学习训练任务和其他
业务一样运行在通用网络硬件设备之上。但是分布
式机器学习训练任务具有自己的特点，如流量矩阵
的确定性以及数据传输的周期性等。因此，针对分
布式机器学习业务设计专用的网络设备和架构，如
超低转发时延交换机、GPU 与网卡的一体化设计
等，也将成为未来的研究热点。此外，光电互联技
术的出现也使得数据中心网络的带宽和容量大幅
提高。可以预料，未来在如何合理地利用这些新型
网络设备来提升网络传输性能方面也将会产生更
多的研究成果。
9 总结
当前，以机器学习为代表的人工智能技术蓬勃
发展，但是随着训练数据量的不断增长和机器学习
模型的日趋复杂，对算力的要求也越来越高。由于
单一计算设备的性能限制，单机训练的时效性非常
差，因此，分布式机器学习成为加速人工智能发展
的重要基石。然而，在大规模分布式机器学习训练
场景下，计算节点之间的数据通信开销非常高，并
且通信时间的增长会导致计算资源的浪费，降低分
布式机器学习系统的扩展性。
26 计 算 机 学 报
本文分析了网络通信之所以会成为分布式机
器学习系统扩展性瓶颈的主要原因，并提出了三种
关键思路用以缓解甚至消除网络性能对分布式机
器学习系统的限制，然后以这三种思路为基础，全
面概述了当前的分布式机器学习系统网络性能优
化工作，并通过归纳总结的方式从多个角度对比了
不同方案的性能。在此基础上，对分布式机器学习
系统网络性能优化研究未来的发展趋势进行了展
望，以期为后续的相关研究提供重要参考。